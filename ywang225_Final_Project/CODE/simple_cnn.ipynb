{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gIa4ACTI9sHs"
   },
   "source": [
    "**Install module**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hKiO3onV_tah"
   },
   "outputs": [],
   "source": [
    "path = '/content/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ImL6wauA9sHt"
   },
   "outputs": [],
   "source": [
    "!pip install imagehash\n",
    "!pip install tqdm\n",
    "!pip install pickle\n",
    "!pip install sys\n",
    "!pip install platform\n",
    "!pip install scikit-image \n",
    "!pip install keras \n",
    "!pip install scipy\n",
    "!unzip /content/train.zip\n",
    "!unzip /content/test.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "96gJ1NNu9sHw"
   },
   "outputs": [],
   "source": [
    "### module\n",
    "\n",
    "# module for read data\n",
    "from pandas import read_csv\n",
    "#from imagehash import phash\n",
    "from math import sqrt\n",
    "import os\n",
    "from os.path import isfile\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image as pil_image\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "from numpy.random import normal as rnorm\n",
    "import random\n",
    "\n",
    "\n",
    "# module for neural network\n",
    "import sys\n",
    "import platform\n",
    "import keras\n",
    "\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import img_to_array,array_to_img\n",
    "from scipy.ndimage import affine_transform\n",
    "\n",
    "from keras.layers import Reshape, Conv2D, Flatten, Dense\n",
    "from keras import regularizers\n",
    "from keras.optimizers import Adam\n",
    "from keras.engine.topology import Input\n",
    "from keras.models import Model\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.applications.mobilenet import MobileNet\n",
    "#from keras.applications.vgg16 import VGG16\n",
    "#from keras.applications.vgg19 import VGG19\n",
    "from keras.layers import Concatenate, Lambda\n",
    "\n",
    "from keras.metrics import categorical_accuracy, top_k_categorical_accuracy, categorical_crossentropy\n",
    "from keras.models import Sequential\n",
    "\n",
    "\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "import numpy as np\n",
    "from keras.utils import Sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zJ0EAdjD9sHy"
   },
   "outputs": [],
   "source": [
    "down_size = 4 # kernal numer // down size: how much kernels to use\n",
    "down_size_data = 5 # len(training_data) // down_size_data  to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M-Y5f_Ic9sH0",
    "outputId": "82d09795-7048-4894-9f8b-8bfcba646e7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['whale-categorization-playground', 'p2h-table-new', 'bounding-box']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.listdir(\"../input/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BfSPya309sH4"
   },
   "outputs": [],
   "source": [
    "def unique(list1): \n",
    "    unique_list = [] \n",
    "    for x in list1: \n",
    "        if x not in unique_list: \n",
    "            unique_list.append(x) \n",
    "    return  unique_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CMZLtWWs9sH6"
   },
   "source": [
    "**Read Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TlyZdUIK9sH6",
    "outputId": "549bb67b-04cd-4426-867e-7885bb6ec02b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n of whales: 4251\n",
      "n of imgs: 9850\n",
      "n of whales: 997\n",
      "n of whales: 199\n",
      "n of imgs: 969\n",
      "n of imgs: 646\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9188d741c7994f5ca11583c6d2338b53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=969), HTML(value='')))"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## read the datasets\n",
    "def expand_path(p):\n",
    "    if isfile(path+'train/' + p): \n",
    "        return path+ 'train/' + p\n",
    "    return p\n",
    "\n",
    "\n",
    "tagged_list = [(p,w) for _,p,w in read_csv(path+'train.csv').to_records()]\n",
    "tagged = dict(tagged_list)\n",
    "\n",
    "all_whales = list( set(tagged.values()) )\n",
    "print(\"n of whales:\",len(all_whales))\n",
    "print(\"n of imgs:\",len(tagged_list))\n",
    "\n",
    "\n",
    "temp = []\n",
    "for i, val in enumerate(all_whales): \n",
    "    if sum([j==val for j in tagged.values()]) > 2:\n",
    "        temp += [val]\n",
    "        \n",
    "print(\"n of whales:\",len(temp))\n",
    "\n",
    "all_whales = random.sample(temp, len(temp) // down_size_data)\n",
    "\n",
    "print(\"n of whales:\",len(all_whales))\n",
    "\n",
    "temp = []\n",
    "for i, val in enumerate(tagged_list): \n",
    "    if val[1] in all_whales:\n",
    "        temp += [val]\n",
    "print(\"n of imgs:\",len(temp))\n",
    "\n",
    "n_train = len(temp) // 3 * 2\n",
    "tagged = dict(temp[:n_train]) # training set\n",
    "submit = dict(temp[n_train:]) # testing set\n",
    "print(\"n of imgs:\",len(tagged))\n",
    "\n",
    "\n",
    "join   = list(tagged.keys()) + list(submit.keys())\n",
    "\n",
    "\n",
    "### Reading other datasets\n",
    "\n",
    "# Read the bounding box data from the bounding box kernel (see reference above)\n",
    "path1 = path + 'bounding-box.pickle'\n",
    "with open(path1, 'rb') as f:\n",
    "    p2bb = pickle.load(f)\n",
    "\n",
    "#Delete duplicate images\n",
    "# numbers: 25460 to 20913\n",
    "# path2 = '../input/h2p-table/h2p_table.csv'\n",
    "# h2p = dict([(p,w) for _,p,w in read_csv(path2).to_records()])\n",
    "# path3 = '../input/p2htable/p2htable.csv'\n",
    "# p2h = dict([(p,w) for _,p,w in read_csv(path3).to_records()])\n",
    "\n",
    "p2size = {}\n",
    "for p in tqdm_notebook(join):\n",
    "    size      = pil_image.open(expand_path(p)).size\n",
    "    p2size[p] = size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6SoeNtVG9sH9"
   },
   "source": [
    "**Divide into train dataset and test dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g9qo83Dm9sH-",
    "outputId": "54582e08-5468-4a49-81a5-18fc706be601"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image</th>\n",
       "      <th>Id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00eab0e6.jpg</td>\n",
       "      <td>w_886257d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00f4c92c.jpg</td>\n",
       "      <td>w_9ea2cc3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>019cd867.jpg</td>\n",
       "      <td>w_0ead9d7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>020028b2.jpg</td>\n",
       "      <td>w_0de84f0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0237ca7a.jpg</td>\n",
       "      <td>w_8ba2066</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Image         Id\n",
       "0  00eab0e6.jpg  w_886257d\n",
       "1  00f4c92c.jpg  w_9ea2cc3\n",
       "2  019cd867.jpg  w_0ead9d7\n",
       "3  020028b2.jpg  w_0de84f0\n",
       "4  0237ca7a.jpg  w_8ba2066"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "#train:dict to list\n",
    "train_new_list=[]\n",
    "for key, value in tagged.items():\n",
    "    temp = [key,value]\n",
    "    train_new_list.append(temp)\n",
    "#train:list to df\n",
    "train_new_df = pd.DataFrame(train_new_list, columns = ['Image','Id']) \n",
    "train_new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yGJQo8zR9sIB",
    "outputId": "979903ec-3983-4dfb-c15a-925518d422d4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image</th>\n",
       "      <th>Id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aaff3333.jpg</td>\n",
       "      <td>w_fc7cc24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ab09fe21.jpg</td>\n",
       "      <td>w_a59905f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abca61c7.jpg</td>\n",
       "      <td>w_cd02407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abcf9835.jpg</td>\n",
       "      <td>w_e6f5e09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abd863e3.jpg</td>\n",
       "      <td>w_e3c119c</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Image         Id\n",
       "0  aaff3333.jpg  w_fc7cc24\n",
       "1  ab09fe21.jpg  w_a59905f\n",
       "2  abca61c7.jpg  w_cd02407\n",
       "3  abcf9835.jpg  w_e6f5e09\n",
       "4  abd863e3.jpg  w_e3c119c"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train:dict to list\n",
    "test_new_list=[]\n",
    "for key, value in submit.items():\n",
    "    temp = [key,value]\n",
    "    test_new_list.append(temp)\n",
    "#train:list to df\n",
    "test_new_df = pd.DataFrame(test_new_list, columns = ['Image','Id']) \n",
    "test_new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GG-Q30nC9sIE"
   },
   "outputs": [],
   "source": [
    "# Suppress annoying stderr output when importing keras.\n",
    "import sys\n",
    "import platform\n",
    "old_stderr = sys.stderr\n",
    "sys.stderr = open('/dev/null' if platform.system() != 'Windows' else 'nul', 'w')\n",
    "import keras\n",
    "sys.stderr = old_stderr\n",
    "\n",
    "import random\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import img_to_array,array_to_img\n",
    "from scipy.ndimage import affine_transform\n",
    "\n",
    "# show imgs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_whale(images, per_row=2):\n",
    "    # input: iamge names['0c35fcb4.jpg'] or names\n",
    "    if type(images[0]) == str:\n",
    "        imgs = [pil_image.open(expand_path(p)) for p in images]\n",
    "    else:\n",
    "        imgs = images\n",
    "    n         = len(imgs)\n",
    "    rows      = (n + per_row - 1)//per_row\n",
    "    cols      = min(per_row, n)\n",
    "    fig, axes = plt.subplots(rows,cols, figsize=(24//per_row*cols,24//per_row*rows))\n",
    "    for ax in axes.flatten(): ax.axis('off')\n",
    "    for i,(img,ax) in enumerate(zip(imgs, axes.flatten())): ax.imshow(img.convert('RGB'))\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ltox-mMp9sIG"
   },
   "outputs": [],
   "source": [
    "# Read the bounding box data from the bounding box kernel (see reference above)\n",
    "#path1 = '../input/bound-box/bounding-box.pickle'\n",
    "#with open(path1, 'rb') as f:\n",
    "#    p2bb = pickle.load(f)\n",
    "\n",
    "#Delete duplicate images\n",
    "# numbers: 25460 to 20913\n",
    "path2 = path + 'h2p_table.csv'\n",
    "h2p = dict([(p,w) for _,p,w in read_csv(path2).to_records()])\n",
    "path3 = path + 'p2htable.csv'\n",
    "p2h = dict([(p,w) for _,p,w in read_csv(path3).to_records()])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6V5Lm_B69sII"
   },
   "source": [
    "**Image Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yICnfSGU9sIJ"
   },
   "outputs": [],
   "source": [
    "# one img name to the img[PIL file]\n",
    "\n",
    "img_shape    = (384,384,1) # The image shape used by the model\n",
    "anisotropy   = 2.15 # The horizontal compression ratio\n",
    "crop_margin  = 0.05 # The margin added around the bounding box to \n",
    "# compensate for bounding box inaccuracy\n",
    "\n",
    "def build_transform(rotation, shear, height_zoom, width_zoom, height_shift, width_shift):\n",
    "    \"\"\"\n",
    "    Build a transformation matrix with the specified characteristics.\n",
    "    \"\"\"\n",
    "    rotation        = np.deg2rad(rotation)\n",
    "    shear           = np.deg2rad(shear)\n",
    "    rotation_matrix = np.array([[np.cos(rotation), np.sin(rotation), 0], [-np.sin(rotation), np.cos(rotation), 0], [0, 0, 1]])\n",
    "    shift_matrix    = np.array([[1, 0, height_shift], [0, 1, width_shift], [0, 0, 1]])\n",
    "    shear_matrix    = np.array([[1, np.sin(shear), 0], [0, np.cos(shear), 0], [0, 0, 1]])\n",
    "    zoom_matrix     = np.array([[1.0/height_zoom, 0, 0], [0, 1.0/width_zoom, 0], [0, 0, 1]])\n",
    "    shift_matrix    = np.array([[1, 0, -height_shift], [0, 1, -width_shift], [0, 0, 1]])\n",
    "    return np.dot(np.dot(rotation_matrix, shear_matrix), np.dot(zoom_matrix, shift_matrix))\n",
    "\n",
    "\n",
    "def read_cropped_image(p, augment):\n",
    "    \"\"\"\n",
    "    @param p : the name of the picture to read\n",
    "    @param augment: True/False if data augmentation should be performed\n",
    "    @return a numpy array with the transformed image\n",
    "    \"\"\"\n",
    "    # If an image id was given (wrongly), convert to filename\n",
    "    if p in h2p: p = h2p[p]\n",
    "    size_x,size_y = p2size[p]\n",
    "    \n",
    "    # Determine the region of the original image we want to capture based on the bounding box.\n",
    "    x0,y0,x1,y1   = p2bb[p]\n",
    "    dx            = x1 - x0\n",
    "    dy            = y1 - y0\n",
    "    x0           -= dx*crop_margin\n",
    "    x1           += dx*crop_margin + 1\n",
    "    y0           -= dy*crop_margin\n",
    "    y1           += dy*crop_margin + 1\n",
    "    if (x0 < 0     ): x0 = 0\n",
    "    if (x1 > size_x): x1 = size_x\n",
    "    if (y0 < 0     ): y0 = 0\n",
    "    if (y1 > size_y): y1 = size_y\n",
    "    dx            = x1 - x0\n",
    "    dy            = y1 - y0\n",
    "    if dx > dy*anisotropy:\n",
    "        dy  = 0.5*(dx/anisotropy - dy)\n",
    "        y0 -= dy\n",
    "        y1 += dy\n",
    "    else:\n",
    "        dx  = 0.5*(dy*anisotropy - dx)\n",
    "        x0 -= dx\n",
    "        x1 += dx\n",
    "\n",
    "    # Generate the transformation matrix\n",
    "    trans = np.array([[1, 0, -0.5*img_shape[0]], [0, 1, -0.5*img_shape[1]], [0, 0, 1]])\n",
    "    trans = np.dot(np.array([[(y1 - y0)/img_shape[0], 0, 0], [0, (x1 - x0)/img_shape[1], 0], [0, 0, 1]]), trans)\n",
    "    if augment:\n",
    "#         During training, data augmentation is performed by adding a random transformation\n",
    "#         that composes zoom, shift, rotation and shear.\n",
    "#         The random transform is skipped when testing.\n",
    "        trans = np.dot(build_transform(\n",
    "            random.uniform(-5, 5),\n",
    "            random.uniform(-5, 5),\n",
    "            random.uniform(0.8, 1.0),\n",
    "            random.uniform(0.8, 1.0),\n",
    "            random.uniform(-0.05*(y1 - y0), 0.05*(y1 - y0)),\n",
    "            random.uniform(-0.05*(x1 - x0), 0.05*(x1 - x0))\n",
    "            ), trans)\n",
    "    trans = np.dot(np.array([[1, 0, 0.5*(y1 + y0)], [0, 1, 0.5*(x1 + x0)], [0, 0, 1]]), trans)\n",
    "\n",
    "    # Read the image, transform to black and white and comvert to numpy array\n",
    "    img   = read_raw_image(p).convert('L')\n",
    "    img   = img_to_array(img)\n",
    "    \n",
    "    # Resize: Apply affine transformation\n",
    "    matrix = trans[:2,:2]\n",
    "    offset = trans[:2,2]\n",
    "    img    = img.reshape(img.shape[:-1])\n",
    "    img    = affine_transform(img, matrix, offset, output_shape=img_shape[:-1], order=1, mode='constant', cval=np.average(img))\n",
    "    img    = img.reshape(img_shape)\n",
    "\n",
    "    # Normalize to zero mean and unit variance\n",
    "    img  -= np.mean(img, keepdims=True)\n",
    "    img  /= np.std(img, keepdims=True) + K.epsilon()\n",
    "    return img\n",
    "\n",
    "def read_raw_image(p):\n",
    "    # name to image \n",
    "    img = pil_image.open(expand_path(p))\n",
    "    return img\n",
    "\n",
    "def read_for_training(p):\n",
    "    return read_cropped_image(p, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wDasExkG9sIL"
   },
   "outputs": [],
   "source": [
    "def whether_new_whale(data,index):\n",
    "    if data['Id'][index] == \"new_whale\":\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def count_whale_id(data):\n",
    "    count = 0\n",
    "    for i in np.arange(0,data.shape[0]):\n",
    "        if whether_new_whale(data,i) == False:\n",
    "            count += 1\n",
    "    return count\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zESUDPjy9sIN",
    "outputId": "4b719fd1-9744-47f0-dcf6-d99de49457ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "969\n",
      "646\n",
      "323\n"
     ]
    }
   ],
   "source": [
    "train_test_df = pd.concat([train_new_df,test_new_df],ignore_index=True)\n",
    "count_whale_train_new = count_whale_id(train_test_df)\n",
    "print(count_whale_train_new) \n",
    "print(count_whale_id(train_new_df))\n",
    "print(count_whale_id(test_new_df))\n",
    "\n",
    "\n",
    "#count_whale_train_new = count_whale_id(train_test_df)\n",
    "#print(count_whale_train_new) \n",
    "#print(count_whale_id(test_new_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9VWc7ccN9sIQ",
    "outputId": "5192f24e-19b7-4a7c-c5e5-f8b2a3aeb1df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199\n",
      "162\n",
      "195\n"
     ]
    }
   ],
   "source": [
    "def num_class(data):\n",
    "    unique_id = pd.unique(data['Id'])\n",
    "    if \"new_whale\"  in unique_id:\n",
    "        return len(unique_id)-1\n",
    "    else:\n",
    "        return(len(unique_id))\n",
    "\n",
    "count_whale_id_all = num_class(train_test_df) \n",
    "print(count_whale_id_all) \n",
    "print(num_class(test_new_df))\n",
    "print(num_class(train_new_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QDdBggBz9sIV"
   },
   "source": [
    "**CNN Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AvvfFKrV9sIX"
   },
   "outputs": [],
   "source": [
    "\n",
    "def top_5_accuracy(y_true, y_pred):\n",
    "    return top_k_categorical_accuracy(y_true, y_pred, k=5)\n",
    "\n",
    "\n",
    "def build_simple_cnn(l2,lr,img_shape=img_shape):\n",
    "    regul = regularizers.l2(l2)\n",
    "    optim = Adam(lr = lr)\n",
    "    input = Input(shape=img_shape) \n",
    "    x = Concatenate()([input,input,input])\n",
    "    base_model = ResNet50(include_top=False,input_tensor=x, weights='imagenet',pooling=\"max\") #output layer 2048 \n",
    "    out = Dense(count_whale_id_all,activation=\"softmax\",name=\"softmax\")(base_model.output)\n",
    "    model = Model(input,out)\n",
    "    model.compile(optim, loss = 'categorical_crossentropy', metrics=[categorical_crossentropy, \n",
    "                                                                     categorical_accuracy,top_5_accuracy])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_iEPYY5g9sIa"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "def prepare_labels(y):    \n",
    "    values = np.array(y)\n",
    "    label_encoder = LabelEncoder()\n",
    "    integer_encoded = label_encoder.fit_transform(values)\n",
    "    # print(integer_encoded)\n",
    "\n",
    "    onehot_encoder = OneHotEncoder(sparse=False)\n",
    "    integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "    onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "    # print(onehot_encoded)\n",
    "\n",
    "    y = onehot_encoded\n",
    "    # print(y.shape)\n",
    "    return y, label_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PTmXtBOa9sId",
    "outputId": "532966c6-67aa-413f-96ad-e4be05d49115"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#select whale_id without new_whale\n",
    "\n",
    "#train:290\n",
    "all_whale_id_train_new = [train_new_df['Id'][i] for i in np.arange(0,train_new_df.shape[0]) if whether_new_whale(train_new_df,i)==False]\n",
    "x_train_new =  [train_new_df['Image'][i] for i in np.arange(0,train_new_df.shape[0]) if whether_new_whale(train_new_df,i)==False]\n",
    "\n",
    "#test:146\n",
    "all_whale_id_test_new = [test_new_df['Id'][i] for i in np.arange(0,test_new_df.shape[0]) if whether_new_whale(test_new_df,i)==False]\n",
    "x_test_new =  [test_new_df['Image'][i] for i in np.arange(0,test_new_df.shape[0]) if whether_new_whale(test_new_df,i)==False]\n",
    "\n",
    "\n",
    "all_whale_id_train_test = all_whale_id_train_new + all_whale_id_test_new\n",
    "all_y, all_label_encoder = prepare_labels(all_whale_id_train_test)\n",
    "y_train_new = all_y[0:len(x_train_new),]\n",
    "y_test_new = all_y[len(x_train_new):,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kMEeWfK09sIj",
    "outputId": "6b38f069-e56d-4537-9320-6ec328a3fd78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(646, 199)\n",
      "(323, 199)\n"
     ]
    }
   ],
   "source": [
    "print(y_train_new.shape)\n",
    "print(y_test_new.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "01utXGms9sIm"
   },
   "source": [
    "**Train CNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AtOZBySc9sIn"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Here, `x_set` is list of path to the images: x\n",
    "# and `y_set` are the associated classes: y\n",
    "\n",
    "img_shape = (384,384,1)\n",
    "\n",
    "class TrainingData(Sequence):\n",
    "    def __init__(self,x_set,y_set,batch_size=32):\n",
    "        super(TrainingData, self).__init__()\n",
    "        #def __init__(self, x_set, y_set, batch_size=32):\n",
    "        self.x, self.y = x_set, y_set  # x:list;y:np.array    \n",
    "        self.batch_size = batch_size\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.x) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        start = self.batch_size*idx\n",
    "        end   = min(start + self.batch_size, len(self.x))\n",
    "        indexes = self.index[start:end]\n",
    "        X_train,y_label = self.__data_generation(indexes)\n",
    "\n",
    "        return X_train, y_label \n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.index = np.arange(len(self.x))\n",
    "        #if self.shuffle == True:\n",
    "        np.random.shuffle(self.index)\n",
    "        \n",
    "    \n",
    "    def __data_generation(self, indexes):\n",
    "        #print(\"data_generation\")\n",
    "        size  = indexes.shape[0]\n",
    "        X_train = np.empty((size,)+img_shape, dtype=K.floatx())\n",
    "        y_label = np.empty((size,self.y.shape[1]))        \n",
    "       \n",
    "        #y_label = self.y[indexes]\n",
    "        for i  in  range(0,size): \n",
    "            #print(indexes[i])\n",
    "            X_train[i,:,:,:] = read_for_training(self.x[indexes[i]])\n",
    "            y_label[i,:] = self.y[indexes[i]]\n",
    "        return X_train, y_label\n",
    "    \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7u1vEcdW9sIp",
    "outputId": "29a8ebc5-097a-4833-85f9-fcc191a3c71a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.2/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "94658560/94653016 [==============================] - 3s 0us/step\n"
     ]
    }
   ],
   "source": [
    "model = build_simple_cnn(0,10e-5) #l2 and lr\n",
    "#try:l2 0 lr 10e-5 good\n",
    "#.      10e-5 10e-5 bad\n",
    "#              10e-8 learn too slow\n",
    "#          0    10e-4 overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2LtLabNs9sIu"
   },
   "source": [
    "**Start Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ESTOQ-xJ9sIv",
    "outputId": "d8c046f1-5dd6-4851-f513-e4999473418e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Epoch 1/100\n",
      "21/21 [==============================] - 35s 2s/step - loss: 15.1436 - categorical_crossentropy: 15.1436 - categorical_accuracy: 0.0239 - top_5_accuracy: 0.0522 - val_loss: 15.1176 - val_categorical_crossentropy: 15.1176 - val_categorical_accuracy: 0.0217 - val_top_5_accuracy: 0.0433\n",
      "Epoch 2/100\n",
      "21/21 [==============================] - 16s 744ms/step - loss: 13.5455 - categorical_crossentropy: 13.5455 - categorical_accuracy: 0.1252 - top_5_accuracy: 0.1536 - val_loss: 14.2668 - val_categorical_crossentropy: 14.2668 - val_categorical_accuracy: 0.0495 - val_top_5_accuracy: 0.1115\n",
      "Epoch 3/100\n",
      "21/21 [==============================] - 17s 811ms/step - loss: 12.3845 - categorical_crossentropy: 12.3845 - categorical_accuracy: 0.2134 - top_5_accuracy: 0.2313 - val_loss: 14.2438 - val_categorical_crossentropy: 14.2438 - val_categorical_accuracy: 0.0526 - val_top_5_accuracy: 0.1176\n",
      "Epoch 4/100\n",
      "21/21 [==============================] - 16s 768ms/step - loss: 12.2033 - categorical_crossentropy: 12.2033 - categorical_accuracy: 0.2236 - top_5_accuracy: 0.2400 - val_loss: 14.0657 - val_categorical_crossentropy: 14.0657 - val_categorical_accuracy: 0.0681 - val_top_5_accuracy: 0.1176\n",
      "Epoch 5/100\n",
      "21/21 [==============================] - 16s 771ms/step - loss: 11.6966 - categorical_crossentropy: 11.6966 - categorical_accuracy: 0.2674 - top_5_accuracy: 0.2734 - val_loss: 14.1734 - val_categorical_crossentropy: 14.1734 - val_categorical_accuracy: 0.0712 - val_top_5_accuracy: 0.1176\n",
      "Epoch 6/100\n",
      "21/21 [==============================] - 17s 795ms/step - loss: 11.7398 - categorical_crossentropy: 11.7398 - categorical_accuracy: 0.2611 - top_5_accuracy: 0.2715 - val_loss: 15.1446 - val_categorical_crossentropy: 15.1446 - val_categorical_accuracy: 0.0217 - val_top_5_accuracy: 0.0836\n",
      "Epoch 7/100\n",
      "21/21 [==============================] - 16s 757ms/step - loss: 11.4716 - categorical_crossentropy: 11.4716 - categorical_accuracy: 0.2730 - top_5_accuracy: 0.2894 - val_loss: 14.3263 - val_categorical_crossentropy: 14.3263 - val_categorical_accuracy: 0.0712 - val_top_5_accuracy: 0.1146\n",
      "Epoch 8/100\n",
      "21/21 [==============================] - 17s 825ms/step - loss: 11.3686 - categorical_crossentropy: 11.3686 - categorical_accuracy: 0.2862 - top_5_accuracy: 0.2907 - val_loss: 13.9854 - val_categorical_crossentropy: 13.9854 - val_categorical_accuracy: 0.0867 - val_top_5_accuracy: 0.1331\n",
      "Epoch 9/100\n",
      "21/21 [==============================] - 16s 758ms/step - loss: 10.9411 - categorical_crossentropy: 10.9411 - categorical_accuracy: 0.2984 - top_5_accuracy: 0.3043 - val_loss: 14.1489 - val_categorical_crossentropy: 14.1489 - val_categorical_accuracy: 0.0650 - val_top_5_accuracy: 0.1207\n",
      "Epoch 10/100\n",
      "21/21 [==============================] - 17s 787ms/step - loss: 10.4891 - categorical_crossentropy: 10.4891 - categorical_accuracy: 0.2713 - top_5_accuracy: 0.3175 - val_loss: 13.9106 - val_categorical_crossentropy: 13.9106 - val_categorical_accuracy: 0.0743 - val_top_5_accuracy: 0.1269\n",
      "Epoch 11/100\n",
      "21/21 [==============================] - 17s 789ms/step - loss: 9.7403 - categorical_crossentropy: 9.7403 - categorical_accuracy: 0.3582 - top_5_accuracy: 0.3686 - val_loss: 13.7376 - val_categorical_crossentropy: 13.7376 - val_categorical_accuracy: 0.0495 - val_top_5_accuracy: 0.1610\n",
      "Epoch 12/100\n",
      "21/21 [==============================] - 16s 777ms/step - loss: 9.6550 - categorical_crossentropy: 9.6550 - categorical_accuracy: 0.3610 - top_5_accuracy: 0.3805 - val_loss: 13.5142 - val_categorical_crossentropy: 13.5142 - val_categorical_accuracy: 0.0619 - val_top_5_accuracy: 0.1548\n",
      "Epoch 13/100\n",
      "21/21 [==============================] - 16s 768ms/step - loss: 9.5688 - categorical_crossentropy: 9.5688 - categorical_accuracy: 0.3748 - top_5_accuracy: 0.3837 - val_loss: 13.3000 - val_categorical_crossentropy: 13.3000 - val_categorical_accuracy: 0.0619 - val_top_5_accuracy: 0.1486\n",
      "Epoch 14/100\n",
      "21/21 [==============================] - 16s 758ms/step - loss: 9.5736 - categorical_crossentropy: 9.5736 - categorical_accuracy: 0.3701 - top_5_accuracy: 0.3805 - val_loss: 13.2740 - val_categorical_crossentropy: 13.2740 - val_categorical_accuracy: 0.0712 - val_top_5_accuracy: 0.1672\n",
      "Epoch 15/100\n",
      "21/21 [==============================] - 17s 797ms/step - loss: 9.7791 - categorical_crossentropy: 9.7791 - categorical_accuracy: 0.3640 - top_5_accuracy: 0.3714 - val_loss: 12.8122 - val_categorical_crossentropy: 12.8122 - val_categorical_accuracy: 0.1084 - val_top_5_accuracy: 0.1734\n",
      "Epoch 16/100\n",
      "21/21 [==============================] - 17s 789ms/step - loss: 9.4044 - categorical_crossentropy: 9.4044 - categorical_accuracy: 0.3837 - top_5_accuracy: 0.3942 - val_loss: 12.8018 - val_categorical_crossentropy: 12.8018 - val_categorical_accuracy: 0.1146 - val_top_5_accuracy: 0.1672\n",
      "Epoch 17/100\n",
      "21/21 [==============================] - 16s 767ms/step - loss: 9.6694 - categorical_crossentropy: 9.6694 - categorical_accuracy: 0.3714 - top_5_accuracy: 0.3744 - val_loss: 12.8356 - val_categorical_crossentropy: 12.8356 - val_categorical_accuracy: 0.1084 - val_top_5_accuracy: 0.1796\n",
      "Epoch 18/100\n",
      "21/21 [==============================] - 16s 767ms/step - loss: 9.5720 - categorical_crossentropy: 9.5720 - categorical_accuracy: 0.3776 - top_5_accuracy: 0.3820 - val_loss: 12.7865 - val_categorical_crossentropy: 12.7865 - val_categorical_accuracy: 0.1176 - val_top_5_accuracy: 0.1858\n",
      "Epoch 19/100\n",
      "21/21 [==============================] - 16s 763ms/step - loss: 9.7002 - categorical_crossentropy: 9.7002 - categorical_accuracy: 0.3682 - top_5_accuracy: 0.3742 - val_loss: 12.8209 - val_categorical_crossentropy: 12.8209 - val_categorical_accuracy: 0.1146 - val_top_5_accuracy: 0.1765\n",
      "Epoch 20/100\n",
      "21/21 [==============================] - 17s 790ms/step - loss: 9.2659 - categorical_crossentropy: 9.2659 - categorical_accuracy: 0.3960 - top_5_accuracy: 0.3990 - val_loss: 12.8003 - val_categorical_crossentropy: 12.8003 - val_categorical_accuracy: 0.1176 - val_top_5_accuracy: 0.1765\n",
      "Epoch 21/100\n",
      "21/21 [==============================] - 17s 812ms/step - loss: 9.5815 - categorical_crossentropy: 9.5815 - categorical_accuracy: 0.3763 - top_5_accuracy: 0.3822 - val_loss: 12.7651 - val_categorical_crossentropy: 12.7651 - val_categorical_accuracy: 0.1238 - val_top_5_accuracy: 0.1796\n",
      "Epoch 22/100\n",
      "21/21 [==============================] - 16s 750ms/step - loss: 9.6274 - categorical_crossentropy: 9.6274 - categorical_accuracy: 0.3744 - top_5_accuracy: 0.3774 - val_loss: 12.7689 - val_categorical_crossentropy: 12.7689 - val_categorical_accuracy: 0.1238 - val_top_5_accuracy: 0.1796\n",
      "Epoch 23/100\n",
      "21/21 [==============================] - 16s 762ms/step - loss: 9.6506 - categorical_crossentropy: 9.6506 - categorical_accuracy: 0.3761 - top_5_accuracy: 0.3835 - val_loss: 12.7514 - val_categorical_crossentropy: 12.7514 - val_categorical_accuracy: 0.1331 - val_top_5_accuracy: 0.1703\n",
      "Epoch 24/100\n",
      "21/21 [==============================] - 16s 771ms/step - loss: 9.4286 - categorical_crossentropy: 9.4286 - categorical_accuracy: 0.3882 - top_5_accuracy: 0.3912 - val_loss: 12.7349 - val_categorical_crossentropy: 12.7349 - val_categorical_accuracy: 0.1300 - val_top_5_accuracy: 0.1827\n",
      "Epoch 25/100\n",
      "21/21 [==============================] - 17s 811ms/step - loss: 9.5452 - categorical_crossentropy: 9.5452 - categorical_accuracy: 0.3820 - top_5_accuracy: 0.3850 - val_loss: 12.7328 - val_categorical_crossentropy: 12.7328 - val_categorical_accuracy: 0.1331 - val_top_5_accuracy: 0.1889\n",
      "Epoch 26/100\n",
      "21/21 [==============================] - 16s 772ms/step - loss: 9.7607 - categorical_crossentropy: 9.7607 - categorical_accuracy: 0.3669 - top_5_accuracy: 0.3714 - val_loss: 12.7217 - val_categorical_crossentropy: 12.7217 - val_categorical_accuracy: 0.1362 - val_top_5_accuracy: 0.1920\n",
      "Epoch 27/100\n",
      "21/21 [==============================] - 17s 817ms/step - loss: 9.3284 - categorical_crossentropy: 9.3284 - categorical_accuracy: 0.3929 - top_5_accuracy: 0.3958 - val_loss: 12.8165 - val_categorical_crossentropy: 12.8165 - val_categorical_accuracy: 0.1269 - val_top_5_accuracy: 0.1889\n",
      "Epoch 28/100\n",
      "21/21 [==============================] - 16s 773ms/step - loss: 9.5137 - categorical_crossentropy: 9.5137 - categorical_accuracy: 0.3822 - top_5_accuracy: 0.3912 - val_loss: 12.7959 - val_categorical_crossentropy: 12.7959 - val_categorical_accuracy: 0.1300 - val_top_5_accuracy: 0.1920\n",
      "Epoch 29/100\n",
      "21/21 [==============================] - 16s 767ms/step - loss: 9.8100 - categorical_crossentropy: 9.8100 - categorical_accuracy: 0.3667 - top_5_accuracy: 0.3742 - val_loss: 12.7743 - val_categorical_crossentropy: 12.7743 - val_categorical_accuracy: 0.1207 - val_top_5_accuracy: 0.1858\n",
      "Epoch 30/100\n",
      "21/21 [==============================] - 17s 818ms/step - loss: 9.4970 - categorical_crossentropy: 9.4970 - categorical_accuracy: 0.3865 - top_5_accuracy: 0.3940 - val_loss: 12.7711 - val_categorical_crossentropy: 12.7711 - val_categorical_accuracy: 0.1269 - val_top_5_accuracy: 0.1858\n",
      "Epoch 31/100\n",
      "21/21 [==============================] - 16s 780ms/step - loss: 9.6683 - categorical_crossentropy: 9.6683 - categorical_accuracy: 0.3610 - top_5_accuracy: 0.3835 - val_loss: 12.9954 - val_categorical_crossentropy: 12.9954 - val_categorical_accuracy: 0.1084 - val_top_5_accuracy: 0.1796\n",
      "Epoch 32/100\n",
      "21/21 [==============================] - 16s 771ms/step - loss: 9.5749 - categorical_crossentropy: 9.5749 - categorical_accuracy: 0.3746 - top_5_accuracy: 0.3835 - val_loss: 13.0017 - val_categorical_crossentropy: 13.0017 - val_categorical_accuracy: 0.1022 - val_top_5_accuracy: 0.1765\n",
      "Epoch 33/100\n",
      "21/21 [==============================] - 16s 777ms/step - loss: 9.6084 - categorical_crossentropy: 9.6084 - categorical_accuracy: 0.3731 - top_5_accuracy: 0.3805 - val_loss: 12.9009 - val_categorical_crossentropy: 12.9009 - val_categorical_accuracy: 0.1146 - val_top_5_accuracy: 0.1765\n",
      "Epoch 34/100\n",
      "21/21 [==============================] - 16s 771ms/step - loss: 9.6029 - categorical_crossentropy: 9.6029 - categorical_accuracy: 0.3761 - top_5_accuracy: 0.3850 - val_loss: 12.9427 - val_categorical_crossentropy: 12.9427 - val_categorical_accuracy: 0.1115 - val_top_5_accuracy: 0.1889\n",
      "Epoch 35/100\n",
      "21/21 [==============================] - 17s 822ms/step - loss: 9.1930 - categorical_crossentropy: 9.1930 - categorical_accuracy: 0.3990 - top_5_accuracy: 0.4094 - val_loss: 13.0126 - val_categorical_crossentropy: 13.0126 - val_categorical_accuracy: 0.1084 - val_top_5_accuracy: 0.1734\n",
      "Epoch 36/100\n",
      "21/21 [==============================] - 16s 778ms/step - loss: 9.6754 - categorical_crossentropy: 9.6754 - categorical_accuracy: 0.3669 - top_5_accuracy: 0.3744 - val_loss: 13.2488 - val_categorical_crossentropy: 13.2488 - val_categorical_accuracy: 0.0805 - val_top_5_accuracy: 0.1672\n",
      "Epoch 37/100\n",
      "21/21 [==============================] - 16s 770ms/step - loss: 9.8013 - categorical_crossentropy: 9.8013 - categorical_accuracy: 0.3638 - top_5_accuracy: 0.3727 - val_loss: 12.9731 - val_categorical_crossentropy: 12.9731 - val_categorical_accuracy: 0.1146 - val_top_5_accuracy: 0.1796\n",
      "Epoch 38/100\n",
      "21/21 [==============================] - 16s 769ms/step - loss: 9.5441 - categorical_crossentropy: 9.5441 - categorical_accuracy: 0.3835 - top_5_accuracy: 0.3880 - val_loss: 13.0087 - val_categorical_crossentropy: 13.0087 - val_categorical_accuracy: 0.1146 - val_top_5_accuracy: 0.1734\n",
      "Epoch 39/100\n",
      "21/21 [==============================] - 16s 780ms/step - loss: 9.4763 - categorical_crossentropy: 9.4763 - categorical_accuracy: 0.3701 - top_5_accuracy: 0.3942 - val_loss: 13.3421 - val_categorical_crossentropy: 13.3421 - val_categorical_accuracy: 0.0867 - val_top_5_accuracy: 0.1579\n",
      "Epoch 40/100\n",
      "21/21 [==============================] - 17s 818ms/step - loss: 9.6320 - categorical_crossentropy: 9.6320 - categorical_accuracy: 0.3612 - top_5_accuracy: 0.3865 - val_loss: 14.0270 - val_categorical_crossentropy: 14.0270 - val_categorical_accuracy: 0.0650 - val_top_5_accuracy: 0.1331\n",
      "Epoch 41/100\n",
      "21/21 [==============================] - 16s 777ms/step - loss: 9.6989 - categorical_crossentropy: 9.6989 - categorical_accuracy: 0.3640 - top_5_accuracy: 0.3833 - val_loss: 13.5237 - val_categorical_crossentropy: 13.5237 - val_categorical_accuracy: 0.0743 - val_top_5_accuracy: 0.1672\n",
      "Epoch 42/100\n",
      "21/21 [==============================] - 16s 769ms/step - loss: 9.6250 - categorical_crossentropy: 9.6250 - categorical_accuracy: 0.3731 - top_5_accuracy: 0.3850 - val_loss: 13.3009 - val_categorical_crossentropy: 13.3009 - val_categorical_accuracy: 0.0805 - val_top_5_accuracy: 0.1734\n",
      "Epoch 43/100\n",
      "21/21 [==============================] - 16s 772ms/step - loss: 9.7070 - categorical_crossentropy: 9.7070 - categorical_accuracy: 0.3640 - top_5_accuracy: 0.3744 - val_loss: 13.4300 - val_categorical_crossentropy: 13.4300 - val_categorical_accuracy: 0.0898 - val_top_5_accuracy: 0.1486\n",
      "Epoch 44/100\n",
      "21/21 [==============================] - 17s 812ms/step - loss: 9.5432 - categorical_crossentropy: 9.5432 - categorical_accuracy: 0.3699 - top_5_accuracy: 0.3925 - val_loss: 13.3744 - val_categorical_crossentropy: 13.3744 - val_categorical_accuracy: 0.0929 - val_top_5_accuracy: 0.1672\n",
      "Epoch 45/100\n",
      "21/21 [==============================] - 17s 832ms/step - loss: 9.5090 - categorical_crossentropy: 9.5090 - categorical_accuracy: 0.3630 - top_5_accuracy: 0.4050 - val_loss: 13.1929 - val_categorical_crossentropy: 13.1929 - val_categorical_accuracy: 0.0898 - val_top_5_accuracy: 0.1641\n",
      "Epoch 46/100\n",
      "21/21 [==============================] - 17s 818ms/step - loss: 9.7044 - categorical_crossentropy: 9.7044 - categorical_accuracy: 0.3403 - top_5_accuracy: 0.3746 - val_loss: 13.6154 - val_categorical_crossentropy: 13.6154 - val_categorical_accuracy: 0.0836 - val_top_5_accuracy: 0.1486\n",
      "Epoch 47/100\n",
      "21/21 [==============================] - 16s 779ms/step - loss: 10.2184 - categorical_crossentropy: 10.2184 - categorical_accuracy: 0.2847 - top_5_accuracy: 0.3703 - val_loss: 13.7208 - val_categorical_crossentropy: 13.7208 - val_categorical_accuracy: 0.0650 - val_top_5_accuracy: 0.1455\n",
      "Epoch 48/100\n",
      "21/21 [==============================] - 16s 769ms/step - loss: 9.6161 - categorical_crossentropy: 9.6161 - categorical_accuracy: 0.3418 - top_5_accuracy: 0.3929 - val_loss: 13.3442 - val_categorical_crossentropy: 13.3442 - val_categorical_accuracy: 0.1022 - val_top_5_accuracy: 0.1703\n",
      "Epoch 49/100\n",
      "21/21 [==============================] - 17s 787ms/step - loss: 9.8306 - categorical_crossentropy: 9.8306 - categorical_accuracy: 0.3343 - top_5_accuracy: 0.3837 - val_loss: 13.5461 - val_categorical_crossentropy: 13.5461 - val_categorical_accuracy: 0.0681 - val_top_5_accuracy: 0.1703\n",
      "Epoch 50/100\n",
      "21/21 [==============================] - 17s 808ms/step - loss: 9.7225 - categorical_crossentropy: 9.7225 - categorical_accuracy: 0.3554 - top_5_accuracy: 0.3822 - val_loss: 13.1114 - val_categorical_crossentropy: 13.1114 - val_categorical_accuracy: 0.1207 - val_top_5_accuracy: 0.1858\n",
      "Epoch 51/100\n",
      "21/21 [==============================] - 17s 788ms/step - loss: 9.6113 - categorical_crossentropy: 9.6113 - categorical_accuracy: 0.3718 - top_5_accuracy: 0.3927 - val_loss: 13.1469 - val_categorical_crossentropy: 13.1469 - val_categorical_accuracy: 0.1146 - val_top_5_accuracy: 0.1950\n",
      "Epoch 52/100\n",
      "21/21 [==============================] - 16s 776ms/step - loss: 9.6892 - categorical_crossentropy: 9.6892 - categorical_accuracy: 0.3654 - top_5_accuracy: 0.3789 - val_loss: 12.9327 - val_categorical_crossentropy: 12.9327 - val_categorical_accuracy: 0.1300 - val_top_5_accuracy: 0.1981\n",
      "Epoch 53/100\n",
      "21/21 [==============================] - 16s 778ms/step - loss: 9.4044 - categorical_crossentropy: 9.4044 - categorical_accuracy: 0.3927 - top_5_accuracy: 0.3986 - val_loss: 12.9273 - val_categorical_crossentropy: 12.9273 - val_categorical_accuracy: 0.1269 - val_top_5_accuracy: 0.2074\n",
      "Epoch 54/100\n",
      "21/21 [==============================] - 17s 796ms/step - loss: 9.6057 - categorical_crossentropy: 9.6057 - categorical_accuracy: 0.3791 - top_5_accuracy: 0.3835 - val_loss: 12.8160 - val_categorical_crossentropy: 12.8160 - val_categorical_accuracy: 0.1331 - val_top_5_accuracy: 0.2136\n",
      "Epoch 55/100\n",
      "21/21 [==============================] - 17s 790ms/step - loss: 9.4335 - categorical_crossentropy: 9.4335 - categorical_accuracy: 0.3880 - top_5_accuracy: 0.3955 - val_loss: 12.7620 - val_categorical_crossentropy: 12.7620 - val_categorical_accuracy: 0.1300 - val_top_5_accuracy: 0.2105\n",
      "Epoch 56/100\n",
      "21/21 [==============================] - 16s 778ms/step - loss: 9.6047 - categorical_crossentropy: 9.6047 - categorical_accuracy: 0.3789 - top_5_accuracy: 0.3863 - val_loss: 12.7298 - val_categorical_crossentropy: 12.7298 - val_categorical_accuracy: 0.1300 - val_top_5_accuracy: 0.2105\n",
      "Epoch 57/100\n",
      "21/21 [==============================] - 16s 767ms/step - loss: 9.2899 - categorical_crossentropy: 9.2899 - categorical_accuracy: 0.3969 - top_5_accuracy: 0.3984 - val_loss: 12.7212 - val_categorical_crossentropy: 12.7212 - val_categorical_accuracy: 0.1238 - val_top_5_accuracy: 0.2074\n",
      "Epoch 58/100\n",
      "21/21 [==============================] - 17s 793ms/step - loss: 9.5875 - categorical_crossentropy: 9.5875 - categorical_accuracy: 0.3789 - top_5_accuracy: 0.3818 - val_loss: 12.7167 - val_categorical_crossentropy: 12.7167 - val_categorical_accuracy: 0.1300 - val_top_5_accuracy: 0.2074\n",
      "Epoch 59/100\n",
      "21/21 [==============================] - 17s 808ms/step - loss: 9.2818 - categorical_crossentropy: 9.2818 - categorical_accuracy: 0.4003 - top_5_accuracy: 0.4063 - val_loss: 12.6690 - val_categorical_crossentropy: 12.6690 - val_categorical_accuracy: 0.1331 - val_top_5_accuracy: 0.2198\n",
      "Epoch 60/100\n",
      "21/21 [==============================] - 16s 776ms/step - loss: 9.3533 - categorical_crossentropy: 9.3533 - categorical_accuracy: 0.3940 - top_5_accuracy: 0.3969 - val_loss: 12.6596 - val_categorical_crossentropy: 12.6596 - val_categorical_accuracy: 0.1331 - val_top_5_accuracy: 0.2198\n",
      "Epoch 61/100\n",
      "21/21 [==============================] - 16s 769ms/step - loss: 9.5461 - categorical_crossentropy: 9.5461 - categorical_accuracy: 0.3835 - top_5_accuracy: 0.3999 - val_loss: 12.6635 - val_categorical_crossentropy: 12.6635 - val_categorical_accuracy: 0.1269 - val_top_5_accuracy: 0.2167\n",
      "Epoch 62/100\n",
      "21/21 [==============================] - 16s 771ms/step - loss: 9.3345 - categorical_crossentropy: 9.3345 - categorical_accuracy: 0.3940 - top_5_accuracy: 0.3999 - val_loss: 12.6709 - val_categorical_crossentropy: 12.6709 - val_categorical_accuracy: 0.1238 - val_top_5_accuracy: 0.2198\n",
      "Epoch 63/100\n",
      "21/21 [==============================] - 17s 790ms/step - loss: 9.6243 - categorical_crossentropy: 9.6243 - categorical_accuracy: 0.3804 - top_5_accuracy: 0.3878 - val_loss: 12.6965 - val_categorical_crossentropy: 12.6965 - val_categorical_accuracy: 0.1207 - val_top_5_accuracy: 0.2198\n",
      "Epoch 64/100\n",
      "21/21 [==============================] - 17s 823ms/step - loss: 9.4679 - categorical_crossentropy: 9.4679 - categorical_accuracy: 0.3955 - top_5_accuracy: 0.3999 - val_loss: 12.6806 - val_categorical_crossentropy: 12.6806 - val_categorical_accuracy: 0.1238 - val_top_5_accuracy: 0.2074\n",
      "Epoch 65/100\n",
      "21/21 [==============================] - 17s 796ms/step - loss: 9.4234 - categorical_crossentropy: 9.4234 - categorical_accuracy: 0.3927 - top_5_accuracy: 0.3956 - val_loss: 12.7056 - val_categorical_crossentropy: 12.7056 - val_categorical_accuracy: 0.1300 - val_top_5_accuracy: 0.2074\n",
      "Epoch 66/100\n",
      "21/21 [==============================] - 16s 784ms/step - loss: 9.4772 - categorical_crossentropy: 9.4772 - categorical_accuracy: 0.3984 - top_5_accuracy: 0.4014 - val_loss: 12.7311 - val_categorical_crossentropy: 12.7311 - val_categorical_accuracy: 0.1331 - val_top_5_accuracy: 0.2136\n",
      "Epoch 67/100\n",
      "21/21 [==============================] - 16s 779ms/step - loss: 9.4100 - categorical_crossentropy: 9.4100 - categorical_accuracy: 0.3971 - top_5_accuracy: 0.3986 - val_loss: 12.7304 - val_categorical_crossentropy: 12.7304 - val_categorical_accuracy: 0.1362 - val_top_5_accuracy: 0.2074\n",
      "Epoch 68/100\n",
      "21/21 [==============================] - 16s 774ms/step - loss: 9.4386 - categorical_crossentropy: 9.4386 - categorical_accuracy: 0.3867 - top_5_accuracy: 0.3956 - val_loss: 12.7179 - val_categorical_crossentropy: 12.7179 - val_categorical_accuracy: 0.1393 - val_top_5_accuracy: 0.2012\n",
      "Epoch 69/100\n",
      "21/21 [==============================] - 17s 818ms/step - loss: 9.3923 - categorical_crossentropy: 9.3923 - categorical_accuracy: 0.3942 - top_5_accuracy: 0.3986 - val_loss: 12.6874 - val_categorical_crossentropy: 12.6874 - val_categorical_accuracy: 0.1393 - val_top_5_accuracy: 0.2012\n",
      "Epoch 70/100\n",
      "21/21 [==============================] - 16s 775ms/step - loss: 9.3813 - categorical_crossentropy: 9.3813 - categorical_accuracy: 0.3927 - top_5_accuracy: 0.3986 - val_loss: 12.6765 - val_categorical_crossentropy: 12.6765 - val_categorical_accuracy: 0.1393 - val_top_5_accuracy: 0.2012\n",
      "Epoch 71/100\n",
      "21/21 [==============================] - 16s 771ms/step - loss: 9.3662 - categorical_crossentropy: 9.3662 - categorical_accuracy: 0.3984 - top_5_accuracy: 0.4074 - val_loss: 12.7052 - val_categorical_crossentropy: 12.7052 - val_categorical_accuracy: 0.1300 - val_top_5_accuracy: 0.2012\n",
      "Epoch 72/100\n",
      "21/21 [==============================] - 16s 764ms/step - loss: 9.3668 - categorical_crossentropy: 9.3668 - categorical_accuracy: 0.3971 - top_5_accuracy: 0.4046 - val_loss: 12.7472 - val_categorical_crossentropy: 12.7472 - val_categorical_accuracy: 0.1331 - val_top_5_accuracy: 0.1981\n",
      "Epoch 73/100\n",
      "21/21 [==============================] - 16s 771ms/step - loss: 9.6199 - categorical_crossentropy: 9.6199 - categorical_accuracy: 0.3833 - top_5_accuracy: 0.3878 - val_loss: 12.7237 - val_categorical_crossentropy: 12.7237 - val_categorical_accuracy: 0.1331 - val_top_5_accuracy: 0.1981\n",
      "Epoch 74/100\n",
      "21/21 [==============================] - 17s 822ms/step - loss: 9.1644 - categorical_crossentropy: 9.1644 - categorical_accuracy: 0.4050 - top_5_accuracy: 0.4154 - val_loss: 12.6799 - val_categorical_crossentropy: 12.6799 - val_categorical_accuracy: 0.1362 - val_top_5_accuracy: 0.2105\n",
      "Epoch 75/100\n",
      "21/21 [==============================] - 16s 771ms/step - loss: 9.4615 - categorical_crossentropy: 9.4615 - categorical_accuracy: 0.3925 - top_5_accuracy: 0.4074 - val_loss: 12.6803 - val_categorical_crossentropy: 12.6803 - val_categorical_accuracy: 0.1362 - val_top_5_accuracy: 0.2136\n",
      "Epoch 76/100\n",
      "21/21 [==============================] - 16s 766ms/step - loss: 9.3139 - categorical_crossentropy: 9.3139 - categorical_accuracy: 0.4001 - top_5_accuracy: 0.4076 - val_loss: 12.6471 - val_categorical_crossentropy: 12.6471 - val_categorical_accuracy: 0.1393 - val_top_5_accuracy: 0.2167\n",
      "Epoch 77/100\n",
      "21/21 [==============================] - 17s 793ms/step - loss: 9.5738 - categorical_crossentropy: 9.5738 - categorical_accuracy: 0.3818 - top_5_accuracy: 0.3938 - val_loss: 12.6073 - val_categorical_crossentropy: 12.6073 - val_categorical_accuracy: 0.1393 - val_top_5_accuracy: 0.2136\n",
      "Epoch 78/100\n",
      "21/21 [==============================] - 16s 767ms/step - loss: 9.4458 - categorical_crossentropy: 9.4458 - categorical_accuracy: 0.3910 - top_5_accuracy: 0.3984 - val_loss: 12.6366 - val_categorical_crossentropy: 12.6366 - val_categorical_accuracy: 0.1362 - val_top_5_accuracy: 0.2167\n",
      "Epoch 79/100\n",
      "21/21 [==============================] - 17s 820ms/step - loss: 9.3447 - categorical_crossentropy: 9.3447 - categorical_accuracy: 0.4016 - top_5_accuracy: 0.4076 - val_loss: 12.6788 - val_categorical_crossentropy: 12.6788 - val_categorical_accuracy: 0.1269 - val_top_5_accuracy: 0.2136\n",
      "Epoch 80/100\n",
      "21/21 [==============================] - 16s 777ms/step - loss: 9.4447 - categorical_crossentropy: 9.4447 - categorical_accuracy: 0.3927 - top_5_accuracy: 0.4016 - val_loss: 12.7047 - val_categorical_crossentropy: 12.7047 - val_categorical_accuracy: 0.1238 - val_top_5_accuracy: 0.2136\n",
      "Epoch 81/100\n",
      "21/21 [==============================] - 16s 782ms/step - loss: 9.2555 - categorical_crossentropy: 9.2555 - categorical_accuracy: 0.4048 - top_5_accuracy: 0.4107 - val_loss: 12.6866 - val_categorical_crossentropy: 12.6866 - val_categorical_accuracy: 0.1238 - val_top_5_accuracy: 0.2136\n",
      "Epoch 82/100\n",
      "21/21 [==============================] - 16s 785ms/step - loss: 9.4545 - categorical_crossentropy: 9.4545 - categorical_accuracy: 0.3969 - top_5_accuracy: 0.4059 - val_loss: 12.6587 - val_categorical_crossentropy: 12.6587 - val_categorical_accuracy: 0.1393 - val_top_5_accuracy: 0.2167\n",
      "Epoch 83/100\n",
      "21/21 [==============================] - 17s 828ms/step - loss: 9.4033 - categorical_crossentropy: 9.4033 - categorical_accuracy: 0.3984 - top_5_accuracy: 0.4014 - val_loss: 12.6722 - val_categorical_crossentropy: 12.6722 - val_categorical_accuracy: 0.1269 - val_top_5_accuracy: 0.2136\n",
      "Epoch 84/100\n",
      "21/21 [==============================] - 17s 818ms/step - loss: 9.1327 - categorical_crossentropy: 9.1327 - categorical_accuracy: 0.4094 - top_5_accuracy: 0.4214 - val_loss: 12.6976 - val_categorical_crossentropy: 12.6976 - val_categorical_accuracy: 0.1362 - val_top_5_accuracy: 0.2136\n",
      "Epoch 85/100\n",
      "21/21 [==============================] - 16s 774ms/step - loss: 9.2858 - categorical_crossentropy: 9.2858 - categorical_accuracy: 0.4074 - top_5_accuracy: 0.4118 - val_loss: 12.6383 - val_categorical_crossentropy: 12.6383 - val_categorical_accuracy: 0.1362 - val_top_5_accuracy: 0.2105\n",
      "Epoch 86/100\n",
      "21/21 [==============================] - 16s 766ms/step - loss: 9.1862 - categorical_crossentropy: 9.1862 - categorical_accuracy: 0.4152 - top_5_accuracy: 0.4227 - val_loss: 12.6861 - val_categorical_crossentropy: 12.6861 - val_categorical_accuracy: 0.1393 - val_top_5_accuracy: 0.2167\n",
      "Epoch 87/100\n",
      "21/21 [==============================] - 16s 768ms/step - loss: 9.3927 - categorical_crossentropy: 9.3927 - categorical_accuracy: 0.4029 - top_5_accuracy: 0.4074 - val_loss: 12.6140 - val_categorical_crossentropy: 12.6140 - val_categorical_accuracy: 0.1455 - val_top_5_accuracy: 0.2229\n",
      "Epoch 88/100\n",
      "21/21 [==============================] - 16s 780ms/step - loss: 9.5320 - categorical_crossentropy: 9.5320 - categorical_accuracy: 0.3938 - top_5_accuracy: 0.3997 - val_loss: 12.5666 - val_categorical_crossentropy: 12.5666 - val_categorical_accuracy: 0.1548 - val_top_5_accuracy: 0.2229\n",
      "Epoch 89/100\n",
      "21/21 [==============================] - 17s 798ms/step - loss: 9.0932 - categorical_crossentropy: 9.0932 - categorical_accuracy: 0.4182 - top_5_accuracy: 0.4242 - val_loss: 12.5564 - val_categorical_crossentropy: 12.5564 - val_categorical_accuracy: 0.1517 - val_top_5_accuracy: 0.2322\n",
      "Epoch 90/100\n",
      "21/21 [==============================] - 16s 767ms/step - loss: 9.2335 - categorical_crossentropy: 9.2335 - categorical_accuracy: 0.4137 - top_5_accuracy: 0.4197 - val_loss: 12.6399 - val_categorical_crossentropy: 12.6399 - val_categorical_accuracy: 0.1393 - val_top_5_accuracy: 0.2291\n",
      "Epoch 91/100\n",
      "21/21 [==============================] - 17s 787ms/step - loss: 9.4443 - categorical_crossentropy: 9.4443 - categorical_accuracy: 0.3999 - top_5_accuracy: 0.4044 - val_loss: 12.6624 - val_categorical_crossentropy: 12.6624 - val_categorical_accuracy: 0.1424 - val_top_5_accuracy: 0.2291\n",
      "Epoch 92/100\n",
      "21/21 [==============================] - 16s 767ms/step - loss: 9.4276 - categorical_crossentropy: 9.4276 - categorical_accuracy: 0.4029 - top_5_accuracy: 0.4118 - val_loss: 12.6811 - val_categorical_crossentropy: 12.6811 - val_categorical_accuracy: 0.1424 - val_top_5_accuracy: 0.2229\n",
      "Epoch 93/100\n",
      "21/21 [==============================] - 17s 792ms/step - loss: 9.2203 - categorical_crossentropy: 9.2203 - categorical_accuracy: 0.4197 - top_5_accuracy: 0.4242 - val_loss: 12.6981 - val_categorical_crossentropy: 12.6981 - val_categorical_accuracy: 0.1424 - val_top_5_accuracy: 0.2260\n",
      "Epoch 94/100\n",
      "21/21 [==============================] - 16s 782ms/step - loss: 9.0742 - categorical_crossentropy: 9.0742 - categorical_accuracy: 0.4212 - top_5_accuracy: 0.4301 - val_loss: 12.6704 - val_categorical_crossentropy: 12.6704 - val_categorical_accuracy: 0.1331 - val_top_5_accuracy: 0.2291\n",
      "Epoch 95/100\n",
      "21/21 [==============================] - 17s 794ms/step - loss: 9.3504 - categorical_crossentropy: 9.3504 - categorical_accuracy: 0.4072 - top_5_accuracy: 0.4131 - val_loss: 12.6286 - val_categorical_crossentropy: 12.6286 - val_categorical_accuracy: 0.1331 - val_top_5_accuracy: 0.2322\n",
      "Epoch 96/100\n",
      "21/21 [==============================] - 16s 770ms/step - loss: 9.1727 - categorical_crossentropy: 9.1727 - categorical_accuracy: 0.4167 - top_5_accuracy: 0.4242 - val_loss: 12.6276 - val_categorical_crossentropy: 12.6276 - val_categorical_accuracy: 0.1424 - val_top_5_accuracy: 0.2322\n",
      "Epoch 97/100\n",
      "21/21 [==============================] - 17s 793ms/step - loss: 9.2652 - categorical_crossentropy: 9.2652 - categorical_accuracy: 0.4165 - top_5_accuracy: 0.4225 - val_loss: 12.6966 - val_categorical_crossentropy: 12.6966 - val_categorical_accuracy: 0.1424 - val_top_5_accuracy: 0.2260\n",
      "Epoch 98/100\n",
      "21/21 [==============================] - 17s 802ms/step - loss: 9.3574 - categorical_crossentropy: 9.3574 - categorical_accuracy: 0.4027 - top_5_accuracy: 0.4131 - val_loss: 12.6874 - val_categorical_crossentropy: 12.6874 - val_categorical_accuracy: 0.1424 - val_top_5_accuracy: 0.2229\n",
      "Epoch 99/100\n",
      "21/21 [==============================] - 17s 793ms/step - loss: 9.3201 - categorical_crossentropy: 9.3201 - categorical_accuracy: 0.4131 - top_5_accuracy: 0.4161 - val_loss: 12.6730 - val_categorical_crossentropy: 12.6730 - val_categorical_accuracy: 0.1424 - val_top_5_accuracy: 0.2229\n",
      "Epoch 100/100\n",
      "21/21 [==============================] - 16s 776ms/step - loss: 9.1586 - categorical_crossentropy: 9.1586 - categorical_accuracy: 0.4212 - top_5_accuracy: 0.4286 - val_loss: 12.6130 - val_categorical_crossentropy: 12.6130 - val_categorical_accuracy: 0.1579 - val_top_5_accuracy: 0.2291\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4c3c220a20>"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_new_data=TrainingData(x_train_new,y_train_new)\n",
    "test_new_data=TrainingData(x_test_new,y_test_new)\n",
    "model.fit_generator(train_new_data,validation_data=test_new_data,epochs=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pMiMLddw9sI3"
   },
   "source": [
    "**Prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3Q1Qs92T9sI3",
    "outputId": "dfb1202c-37f3-4d8b-ade1-169da07b3459"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10.19494342803955, 10.19494342803955, 0.28125, 0.40625]"
      ]
     },
     "execution_count": 23,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate_generator(test_new_data,steps=1)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "simple_cnn_final.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
