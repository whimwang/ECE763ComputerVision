{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2 ECE 763 Computer Vision \n",
    "## Author:\n",
    "Yiming Wang, ywang225@ncsu.edu\n",
    "## Aim: \n",
    "1. Use Neural Network including Feedforward NN and LeNet5, a special CNN,  to do face and nonface images classification\n",
    "2. Babysitting the process and tune parameters.\n",
    "\n",
    "## Data Preparation:\n",
    "1. Download Face images [here](http://vis-www.cs.umass.edu/fddb/) and crop the face on my own and divide into Train and Test data set\n",
    "2. Crop nonface images  from the background of the images and divide into Train and Test data set.\n",
    "\n",
    "\n",
    "## Reference: \n",
    "1. Babysitting process<br>\n",
    "[231n Convolutional Neural Network](http://cs231n.github.io)<br>\n",
    "[Code Example](https://medium.com/udacity-pytorch-challengers/ideas-on-how-to-fine-tune-a-pre-trained-model-in-pytorch-184c47185a20)\n",
    "\n",
    "2. Convolutional Neural Networks<br>\n",
    "[Code Example](https://blog.algorithmia.com/convolutional-neural-nets-in-pytorch/)<br>\n",
    "[Tutorial](https://pytorch.org/docs/stable/optim.html)<br>\n",
    "[Image classification](https://medium.com/datadriveninvestor/creating-a-pytorch-image-classifier-da9db139ba80)<br>\n",
    "[CNN Image classification](https://medium.com/@vivekvscool/image-classification-cnn-with-pytorch-5b2cb9ef9476)<br>\n",
    "3. LeNet5<br>\n",
    "[Tutorial](https://engmrk.com/lenet-5-a-classic-cnn-architecture/)<br>\n",
    "4. Jupyter Notebook<br>\n",
    "[Tutorial](https://jupyternotebook.readthedocs.io/en/stable/examples/Notebook/Working%20With%20Markdown%20Cells.html)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.image as mpimg \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import torch.utils.data\n",
    "import torchvision.datasets\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import copy    \n",
    "import time\n",
    "import numpy as np\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load images and prepocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.getcwd()\n",
    "#os.chdir(\"Documents/ncsu course/ncsu 2019 spring/ECE/Project 2/\")\n",
    "#os.chdir(\"..\")\n",
    "\n",
    "\n",
    "assert(os.getcwd()==\"/Users/wangyiming/Documents/ncsu course/ncsu 2019 spring/ECE/Project 2\")\n",
    "resolution = 60\n",
    "\n",
    "\n",
    "Train_root = \"resolution\"+str(resolution)+\"by\"+str(resolution)+\"/extracted_pics/Train/\"\n",
    "Test_root = \"resolution\"+str(resolution)+\"by\"+str(resolution)+\"/extracted_pics/Test/\"\n",
    "Train = os.listdir(Train_root)\n",
    "Test = os.listdir(Test_root)\n",
    "\n",
    "#without normalization for simple feedfordward neural network\n",
    "loader1=transforms.ToTensor()# if not normalize then in range[0,1]\n",
    "\n",
    "#normalization for simple feedforward neural network\n",
    "loader2=transforms.Compose(\n",
    "    [transforms.ToTensor(),#convert an image to tensor\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "#without normalization for LeNet5\n",
    "loader3=transforms.Compose(\n",
    "        [transforms.Resize((32,32)),\n",
    "         transforms.ToTensor()])\n",
    "\n",
    "#normalization for LeNet5\n",
    "loader4=transforms.Compose(\n",
    "        [transforms.Resize((32,32)),\n",
    "         transforms.ToTensor(),\n",
    "         transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))])\n",
    "\n",
    "def load_images_flow(batch_size,root,which_trans):\n",
    "    if(which_trans == 1):\n",
    "         transform = loader1\n",
    "    if(which_trans == 2):\n",
    "         transform = loader2\n",
    "    if(which_trans == 3):\n",
    "         transform = loader3\n",
    "    if(which_trans == 4):\n",
    "         transform = loader4\n",
    "    \n",
    "    train_set = torchvision.datasets.ImageFolder(root=root, transform=transform)\n",
    "    train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    \n",
    "    return train_loader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createLossAndOptimizer(net, learning_rate = 0.001, weight_decay = 0, loss_method = \"SGD\"):#weight_decay: tuning parameter of L2 term\n",
    "     #Loss function\n",
    "    loss = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    #Optimizer\n",
    "    if loss_method == \"SGD\":\n",
    "        optimizer = optim.SGD(net.parameters(), lr=learning_rate, weight_decay=weight_decay) \n",
    "    if loss_method == \"Adam\":\n",
    "        optimizer = optim.Adam(net.parameters(), lr=learning_rate, weight_decay=weight_decay) \n",
    "    return(loss, optimizer)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Feedforward Neural Network and LeNet5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 3*60*60; hidden_size = 50; output_size = 2   \n",
    "class Two_Layers_NN(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Two_Layers_NN,self).__init__()\n",
    "       \n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "    \n",
    "\n",
    "     \n",
    "\n",
    "class LeNet(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(LeNet,self).__init__()\n",
    "        \n",
    "        self.conv1 = torch.nn.Conv2d(3, 6, kernel_size=(5,5), stride=1)#input 3 channels, output 6 channels\n",
    "        \n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=(2,2),stride=2)\n",
    "        # torch.nn.AdaptiveAvgPool2d() can avoid overfitting\n",
    "        self.conv2 = torch.nn.Conv2d(6, 16, kernel_size=(5,5), stride=1)#output may not be the times of input\n",
    "        \n",
    "        \n",
    "        self.fc1 = torch.nn.Linear(5*5*16,120)\n",
    "        \n",
    "        self.fc2 = torch.nn.Linear(120,84)\n",
    "        \n",
    "        self.fc3 = torch.nn.Linear(84,2)\n",
    "\n",
    "  \n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x) #input 32*32*3 output 28*28*6\n",
    "       \n",
    "        #x = self.batchnorm1(x)\n",
    "        \n",
    "        out = self.maxpool(out) #output 14*14*6\n",
    "    \n",
    "        out = self.conv2(out) #output 10*10*16\n",
    " \n",
    "        out = self.maxpool(out) #output 5*5*16\n",
    "  \n",
    "        out = out.view(-1, 5 * 5 * 16)#flatten\n",
    "   \n",
    "        out = self.fc1(out)\n",
    "  \n",
    "        out = self.fc2(out)\n",
    "      \n",
    "        out = self.fc3(out)\n",
    "        return(out)\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_net(net, loss_method, which_model, whether_norm, batch_size, n_epochs, learning_rate, weight_decay, print_train_process = True, print_test_process= True, validation = True):\n",
    "    assert(which_model == \"NN\" or which_model == \"LeNet\")# the input should be reasonable\n",
    "    \n",
    "    #choose right transformation\n",
    "    if (whether_norm == False and which_model == \"NN\"):\n",
    "        which_trans = 1\n",
    "    elif (whether_norm == True and which_model == \"NN\"):\n",
    "        which_trans = 2\n",
    "    elif (whether_norm == False and which_model == \"LeNet\"):\n",
    "        which_trans = 3\n",
    "    elif (whether_norm == True and which_model == \"LeNet\"):\n",
    "        which_trans = 4\n",
    "        \n",
    "    #print(which_trans)\n",
    "        \n",
    "    #Get training data and test data\n",
    "    train_loader = load_images_flow(batch_size, Train_root, which_trans)\n",
    "    test_loader = load_images_flow(batch_size, Test_root, which_trans)\n",
    "     \n",
    "    \n",
    "    n_batches = len(train_loader)\n",
    "    loss, optimizer =  createLossAndOptimizer(net, learning_rate = learning_rate, weight_decay = weight_decay,loss_method=loss_method)\n",
    "    \n",
    "    #Time for printing\n",
    "    start_time = time.time()\n",
    "    print_every = n_batches // 10\n",
    "        \n",
    "    for epoch in range(n_epochs):\n",
    "        #epoch = 0\n",
    "        running_loss = 0\n",
    "        running_correct_num = 0\n",
    "        \n",
    "        total_train_loss = 0\n",
    "        total_train_num = 0\n",
    "        total_correct_train_num = 0\n",
    "                \n",
    "        for i, data in enumerate(train_loader): # handle every batch_size pictures\n",
    "             \n",
    "            (inputs,labels) = data\n",
    "            \n",
    "            if(which_model == \"NN\"):\n",
    "                 inputs = inputs.view(inputs.size()[0],3*60*60)#flatten\n",
    "                 #inputs = inputs.view(batch_size,3*60*60) not batch size since \n",
    "            \n",
    "            inputs, labels = Variable(inputs), Variable(labels)\n",
    "            \n",
    "            optimizer.zero_grad() # whether zero setting is okay ?\n",
    "            #print(inputs.size())\n",
    "            #Forward pass, backward pass, optimize\n",
    "            outputs = net(inputs) #why ? same as forward\n",
    "            \n",
    "            m, predicted = torch.max(outputs.data,1)\n",
    "            total_train_num += labels.size(0)\n",
    "            running_correct_num += (predicted == labels).sum().item()\n",
    "            total_correct_train_num += (predicted == labels).sum().item()\n",
    "            \n",
    "            loss_size = loss(outputs, labels)\n",
    "            if np.isnan(loss_size.data):\n",
    "                raise ValueError(\"loss explode due to large regularization or learning rate\")\n",
    "\n",
    "            loss_size.backward()\n",
    "            optimizer.step()       \n",
    "            \n",
    "            #print statistics\n",
    "            running_loss += loss_size.data\n",
    "            total_train_loss += loss_size.data\n",
    "              \n",
    "            #print every 10th batch\n",
    "            if(print_train_process == True):\n",
    "                  if (i+1) % (print_every) == 0:\n",
    "                        print(\"Epoch {}, {:d}% \\t train_loss: {:.4f} train_accuracy:{:d}% took: {:.2f}s\".format(\n",
    "                             epoch+1, int(100*(i+1)/n_batches), running_loss/print_every/batch_size, int(100 * running_correct_num /print_every/batch_size),\n",
    "                             time.time()-start_time)) # loss for currect running_loss and running_correct_num not accumulated ones\n",
    "                        #reset running loss and time\n",
    "                        running_loss = 0.0\n",
    "                        running_correct_num = 0.0\n",
    "                        start_time = time.time()\n",
    "                        \n",
    "                                                \n",
    "        #For validation\n",
    "        if(validation == True):       \n",
    "            total_test_loss = 0\n",
    "            total_test_num = 0\n",
    "            correct_test_num = 0\n",
    "            for inputs, labels in test_loader:  \n",
    "                if (which_model == \"NN\"):\n",
    "                    inputs = inputs.view(inputs.size()[0], 3*60*60)\n",
    "                    inputs, labels = Variable(inputs), Variable(labels)\n",
    "        \n",
    "                #Forward pass\n",
    "                test_outputs = net(inputs)\n",
    "        \n",
    "                #test accuracy rate\n",
    "                m, predicted = torch.max(test_outputs.data, 1)\n",
    "                #print(predicted)\n",
    "                total_test_num += labels.size(0)\n",
    "                \n",
    "                correct_test_num += (predicted == labels).sum().item()\n",
    "                test_loss_size=loss(test_outputs, labels)\n",
    "                total_test_loss += test_loss_size.data\n",
    "                                \n",
    "            if(print_test_process == True):        \n",
    "                print(\"Test loss = {:.4f} Test Accuracy = {:d}%\".format(total_test_loss / len(test_loader), \n",
    "                       int(100 * correct_test_num / total_test_num)))\n",
    "        #print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct_test_num / total_test_num))\n",
    "            elif(epoch == n_epochs-1):\n",
    "                 print(\"Test loss = {:.4f} Test Accuracy = {:d}%\".format(total_test_loss / len(test_loader), \n",
    "                       int(100 * correct_test_num / total_test_num)))\n",
    "            \n",
    "            #print(\"Training finished, took {:.2f}s\".format(time.time() - start_time))\n",
    "           \n",
    "    if(not print_train_process == True):\n",
    "          print(\"train_loss: {:.8f} train_accuracy:{:d}% learning rate:{:.8f} regularization:{:.8f} running time:{:.4f}\" .format(running_loss/total_train_num, int(100 * total_correct_train_num / total_train_num),\n",
    "           learning_rate, weight_decay, time.time()-start_time))  \n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the results with and without normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight decay:0.0000 learning rate:0.0010\n",
      "Epoch 1, 10% \t train_loss: 0.1167 train_accuracy:73% took: 0.62s\n",
      "Epoch 1, 20% \t train_loss: 0.0894 train_accuracy:81% took: 0.53s\n",
      "Epoch 1, 30% \t train_loss: 0.0842 train_accuracy:82% took: 0.55s\n",
      "Epoch 1, 40% \t train_loss: 0.0770 train_accuracy:85% took: 0.54s\n",
      "Epoch 1, 50% \t train_loss: 0.0704 train_accuracy:86% took: 0.51s\n",
      "Epoch 1, 60% \t train_loss: 0.0728 train_accuracy:86% took: 0.50s\n",
      "Epoch 1, 70% \t train_loss: 0.0635 train_accuracy:89% took: 0.52s\n",
      "Epoch 1, 80% \t train_loss: 0.0587 train_accuracy:90% took: 0.51s\n",
      "Epoch 1, 90% \t train_loss: 0.0585 train_accuracy:92% took: 0.50s\n",
      "Epoch 1, 100% \t train_loss: 0.0558 train_accuracy:92% took: 0.51s\n"
     ]
    }
   ],
   "source": [
    "weight_decay = 0; learning_rate = 0.001\n",
    "print(\"weight decay:{:.4f} learning rate:{:.4f}\".format(weight_decay, learning_rate))\n",
    "NN = Two_Layers_NN()\n",
    "train_net(NN, which_model = \"NN\", whether_norm = False, batch_size=5, n_epochs=1, learning_rate = learning_rate, \n",
    "              weight_decay = weight_decay, print_train_process = True, print_test_process = False, validation = False,loss_method = \"SGD\")\n",
    "del NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without normalization, optimization converge slow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight decay:0.0000 learning rate:0.0010\n",
      "Epoch 1, 10% \t train_loss: 0.1054 train_accuracy:72% took: 0.66s\n",
      "Epoch 1, 20% \t train_loss: 0.0902 train_accuracy:78% took: 0.55s\n",
      "Epoch 1, 30% \t train_loss: 0.0783 train_accuracy:85% took: 0.64s\n",
      "Epoch 1, 40% \t train_loss: 0.0741 train_accuracy:84% took: 0.54s\n",
      "Epoch 1, 50% \t train_loss: 0.0733 train_accuracy:85% took: 0.64s\n",
      "Epoch 1, 60% \t train_loss: 0.0706 train_accuracy:86% took: 0.58s\n",
      "Epoch 1, 70% \t train_loss: 0.0624 train_accuracy:89% took: 0.54s\n",
      "Epoch 1, 80% \t train_loss: 0.0659 train_accuracy:87% took: 0.51s\n",
      "Epoch 1, 90% \t train_loss: 0.0594 train_accuracy:89% took: 0.52s\n",
      "Epoch 1, 100% \t train_loss: 0.0610 train_accuracy:90% took: 0.53s\n",
      "weight decay:1000.0000 learning rate:0.0010\n",
      "Epoch 1, 10% \t train_loss: 0.1386 train_accuracy:49% took: 0.96s\n",
      "Epoch 1, 20% \t train_loss: 0.1386 train_accuracy:51% took: 0.54s\n",
      "Epoch 1, 30% \t train_loss: 0.1386 train_accuracy:50% took: 0.56s\n",
      "Epoch 1, 40% \t train_loss: 0.1386 train_accuracy:51% took: 0.55s\n",
      "Epoch 1, 50% \t train_loss: 0.1386 train_accuracy:50% took: 0.52s\n",
      "Epoch 1, 60% \t train_loss: 0.1386 train_accuracy:52% took: 0.52s\n",
      "Epoch 1, 70% \t train_loss: 0.1386 train_accuracy:52% took: 0.56s\n",
      "Epoch 1, 80% \t train_loss: 0.1386 train_accuracy:47% took: 0.66s\n",
      "Epoch 1, 90% \t train_loss: 0.1386 train_accuracy:49% took: 0.54s\n",
      "Epoch 1, 100% \t train_loss: 0.1386 train_accuracy:53% took: 0.54s\n",
      "weight decay:10000.0000 learning rate:0.0010\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "loss explode due to large regularization or learning rate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-21d314271fff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mNN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTwo_Layers_NN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m train_net(NN, which_model = \"NN\", whether_norm = True, batch_size=5, n_epochs=1, learning_rate = learning_rate, \n\u001b[0;32m---> 24\u001b[0;31m               weight_decay = weight_decay, print_train_process = True, print_test_process = False, validation = False, loss_method = \"SGD\")\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mNN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-373f73c14a16>\u001b[0m in \u001b[0;36mtrain_net\u001b[0;34m(net, loss_method, which_model, whether_norm, batch_size, n_epochs, learning_rate, weight_decay, print_train_process, print_test_process, validation)\u001b[0m\n\u001b[1;32m    207\u001b[0m             \u001b[0mloss_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_size\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loss explode due to large regularization or learning rate\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m             \u001b[0mloss_size\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: loss explode due to large regularization or learning rate"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "weight_decay = 0; learning_rate = 0.001\n",
    "print(\"weight decay:{:.4f} learning rate:{:.4f}\".format(weight_decay, learning_rate))\n",
    "NN = Two_Layers_NN()\n",
    "train_net(NN, which_model = \"NN\", whether_norm = True, batch_size=5, n_epochs=1, learning_rate = learning_rate, \n",
    "              weight_decay = weight_decay, print_train_process = True, print_test_process = False, validation = False,loss_method = \"SGD\")\n",
    "    \n",
    "del NN\n",
    "\n",
    "\n",
    "weight_decay = 1000; learning_rate = 0.001\n",
    "print(\"weight decay:{:.4f} learning rate:{:.4f}\".format(weight_decay, learning_rate))\n",
    "NN = Two_Layers_NN()\n",
    "train_net(NN, which_model = \"NN\", whether_norm = True, batch_size=5, n_epochs=1, learning_rate = learning_rate, \n",
    "              weight_decay = weight_decay, print_train_process = True, print_test_process = False, validation = False, loss_method = \"SGD\")\n",
    "    \n",
    "del NN\n",
    "\n",
    "weight_decay = 10000; learning_rate = 0.001\n",
    "print(\"weight decay:{:.4f} learning rate:{:.4f}\".format(weight_decay, learning_rate))\n",
    "NN = Two_Layers_NN()\n",
    "train_net(NN, which_model = \"NN\", whether_norm = True, batch_size=5, n_epochs=1, learning_rate = learning_rate, \n",
    "              weight_decay = weight_decay, print_train_process = True, print_test_process = False, validation = False, loss_method = \"SGD\")\n",
    "    \n",
    "del NN\n",
    "#comments:  when regularization increase, loss goes up and when regularization is too large, loss explode\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When regularization increases from 0 to 1000, loss also increases. And when regularization is 10000, loss explodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight decay:0.0010 learning rate:0.00000100\n",
      "Epoch 1, 10% \t train_loss: 0.1412 train_accuracy:48% took: 0.63s\n",
      "Epoch 1, 20% \t train_loss: 0.1392 train_accuracy:50% took: 0.56s\n",
      "Epoch 1, 30% \t train_loss: 0.1405 train_accuracy:49% took: 0.69s\n",
      "Epoch 1, 40% \t train_loss: 0.1384 train_accuracy:52% took: 0.68s\n",
      "Epoch 1, 50% \t train_loss: 0.1382 train_accuracy:55% took: 0.54s\n",
      "Epoch 1, 60% \t train_loss: 0.1377 train_accuracy:53% took: 0.56s\n",
      "Epoch 1, 70% \t train_loss: 0.1372 train_accuracy:56% took: 0.55s\n",
      "Epoch 1, 80% \t train_loss: 0.1374 train_accuracy:55% took: 0.57s\n",
      "Epoch 1, 90% \t train_loss: 0.1355 train_accuracy:58% took: 0.55s\n",
      "Epoch 1, 100% \t train_loss: 0.1353 train_accuracy:58% took: 0.55s\n",
      "weight decay:0.0010 learning rate:0.00100000\n",
      "Epoch 1, 10% \t train_loss: 0.1112 train_accuracy:68% took: 0.59s\n",
      "Epoch 1, 20% \t train_loss: 0.0876 train_accuracy:77% took: 0.70s\n",
      "Epoch 1, 30% \t train_loss: 0.0839 train_accuracy:79% took: 0.51s\n",
      "Epoch 1, 40% \t train_loss: 0.0801 train_accuracy:81% took: 0.52s\n",
      "Epoch 1, 50% \t train_loss: 0.0776 train_accuracy:81% took: 0.74s\n",
      "Epoch 1, 60% \t train_loss: 0.0740 train_accuracy:83% took: 0.57s\n",
      "Epoch 1, 70% \t train_loss: 0.0715 train_accuracy:83% took: 0.61s\n",
      "Epoch 1, 80% \t train_loss: 0.0639 train_accuracy:87% took: 0.55s\n",
      "Epoch 1, 90% \t train_loss: 0.0629 train_accuracy:86% took: 0.56s\n",
      "Epoch 1, 100% \t train_loss: 0.0624 train_accuracy:87% took: 0.52s\n",
      "weight decay:0.0010 learning rate:0.10000000\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "loss explode due to large regularization or learning rate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-3abc2a5cbdeb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mNN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTwo_Layers_NN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m train_net(NN, which_model = \"NN\", whether_norm = True, batch_size=5, n_epochs=1, learning_rate = learning_rate, \n\u001b[0;32m---> 21\u001b[0;31m               weight_decay = weight_decay, print_train_process = True, print_test_process = False, validation = False, loss_method = \"SGD\")\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mNN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-373f73c14a16>\u001b[0m in \u001b[0;36mtrain_net\u001b[0;34m(net, loss_method, which_model, whether_norm, batch_size, n_epochs, learning_rate, weight_decay, print_train_process, print_test_process, validation)\u001b[0m\n\u001b[1;32m    207\u001b[0m             \u001b[0mloss_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_size\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loss explode due to large regularization or learning rate\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m             \u001b[0mloss_size\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: loss explode due to large regularization or learning rate"
     ]
    }
   ],
   "source": [
    "\n",
    "weight_decay = 0.001; learning_rate = 0.000001\n",
    "print(\"weight decay:{:.4f} learning rate:{:.8f}\".format(weight_decay, learning_rate))\n",
    "NN = Two_Layers_NN()\n",
    "train_net(NN, which_model = \"NN\", whether_norm = True, batch_size=5, n_epochs=1, learning_rate = learning_rate, \n",
    "              weight_decay = weight_decay, print_train_process = True, print_test_process = False, validation = False, loss_method = \"SGD\")\n",
    "del NN\n",
    "\n",
    "weight_decay = 0.001; learning_rate = 0.001\n",
    "print(\"weight decay:{:.4f} learning rate:{:.8f}\".format(weight_decay, learning_rate))\n",
    "NN = Two_Layers_NN()\n",
    "train_net(NN, which_model = \"NN\", whether_norm = True, batch_size=5, n_epochs=1, learning_rate = learning_rate, \n",
    "              weight_decay = weight_decay, print_train_process = True, print_test_process = False, validation = False, loss_method = \"SGD\")\n",
    "    \n",
    "del NN\n",
    "\n",
    "weight_decay = 0.001; learning_rate = 0.1\n",
    "print(\"weight decay:{:.4f} learning rate:{:.8f}\".format(weight_decay, learning_rate))\n",
    "NN = Two_Layers_NN()\n",
    "train_net(NN, which_model = \"NN\", whether_norm = True, batch_size=5, n_epochs=1, learning_rate = learning_rate, \n",
    "              weight_decay = weight_decay, print_train_process = True, print_test_process = False, validation = False, loss_method = \"SGD\")\n",
    "    \n",
    "del NN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When learning rate is 0.000001(too small), loss barely changes.\n",
    "When learning rate is 0.001, loss changes reasonably.\n",
    "When learning rate is 0.1, loss explodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter optimization (random search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss = 0.1346 Test Accuracy = 95%\n",
      "train_loss: 0.03103231 train_accuracy:94% learning rate:0.01331377 regularization:0.00002323 running time:28.9505\n",
      "Test loss = 0.1417 Test Accuracy = 95%\n",
      "train_loss: 0.03743628 train_accuracy:93% learning rate:0.02483877 regularization:0.00006424 running time:28.2589\n",
      "Test loss = 0.3235 Test Accuracy = 93%\n",
      "train_loss: 0.07033894 train_accuracy:89% learning rate:0.00026566 regularization:0.41746897 running time:27.7455\n",
      "Test loss = 0.4991 Test Accuracy = 71%\n",
      "train_loss: 0.10152265 train_accuracy:72% learning rate:0.00000938 regularization:0.00000534 running time:27.8754\n",
      "Test loss = 0.2857 Test Accuracy = 91%\n",
      "train_loss: 0.06496082 train_accuracy:88% learning rate:0.00012364 regularization:0.00941348 running time:31.7366\n",
      "Test loss = 0.5141 Test Accuracy = 76%\n",
      "train_loss: 0.10385424 train_accuracy:75% learning rate:0.00000476 regularization:0.00000144 running time:30.7335\n",
      "Test loss = 0.1700 Test Accuracy = 94%\n",
      "train_loss: 0.05674806 train_accuracy:89% learning rate:0.02808510 regularization:0.02969595 running time:28.8332\n",
      "Test loss = 0.2654 Test Accuracy = 93%\n",
      "train_loss: 0.05979075 train_accuracy:91% learning rate:0.00014072 regularization:0.00003963 running time:29.9148\n",
      "loss explodes. learning rate:0.07727178 regularization:0.00005829\n",
      "Test loss = 0.1697 Test Accuracy = 95%\n",
      "train_loss: 0.04097901 train_accuracy:94% learning rate:0.00052046 regularization:0.00830640 running time:27.9550\n"
     ]
    }
   ],
   "source": [
    "for count in range(10):\n",
    "    learning_rate = 10 ** random.uniform(-6,-1)\n",
    "    weight_decay = 10 ** random.uniform(-6,0)\n",
    "    \n",
    "    NN = Two_Layers_NN()\n",
    "    \n",
    "    try:\n",
    "        train_net(NN, which_model = \"NN\", whether_norm = True, batch_size=5, n_epochs=5, learning_rate = learning_rate, \n",
    "              weight_decay = weight_decay, print_train_process = False, print_test_process = False, validation = True,loss_method = \"SGD\")\n",
    "    except:\n",
    "        print(\"loss explodes. learning rate:{:.8f} regularization:{:.8f}\".format(learning_rate, weight_decay))\n",
    "        \n",
    "    del NN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When learning rate:0.00052046 regularization:0.00830640, train loss is the smallest and accuracy is largest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, 10% \t train_loss: 0.1118 train_accuracy:69% took: 0.59s\n",
      "Epoch 1, 20% \t train_loss: 0.0951 train_accuracy:74% took: 0.55s\n",
      "Epoch 1, 30% \t train_loss: 0.0901 train_accuracy:76% took: 0.53s\n",
      "Epoch 1, 40% \t train_loss: 0.0888 train_accuracy:78% took: 0.52s\n",
      "Epoch 1, 50% \t train_loss: 0.0851 train_accuracy:80% took: 0.53s\n",
      "Epoch 1, 60% \t train_loss: 0.0826 train_accuracy:82% took: 0.53s\n",
      "Epoch 1, 70% \t train_loss: 0.0778 train_accuracy:82% took: 0.52s\n",
      "Epoch 1, 80% \t train_loss: 0.0739 train_accuracy:84% took: 0.52s\n",
      "Epoch 1, 90% \t train_loss: 0.0712 train_accuracy:86% took: 0.53s\n",
      "Epoch 1, 100% \t train_loss: 0.0681 train_accuracy:86% took: 0.53s\n",
      "Test loss = 0.3191 Test Accuracy = 85%\n",
      "Epoch 2, 10% \t train_loss: 0.0695 train_accuracy:86% took: 0.73s\n",
      "Epoch 2, 20% \t train_loss: 0.0628 train_accuracy:88% took: 0.54s\n",
      "Epoch 2, 30% \t train_loss: 0.0626 train_accuracy:88% took: 0.53s\n",
      "Epoch 2, 40% \t train_loss: 0.0583 train_accuracy:89% took: 0.54s\n",
      "Epoch 2, 50% \t train_loss: 0.0656 train_accuracy:88% took: 0.52s\n",
      "Epoch 2, 60% \t train_loss: 0.0656 train_accuracy:87% took: 0.70s\n",
      "Epoch 2, 70% \t train_loss: 0.0615 train_accuracy:89% took: 0.62s\n",
      "Epoch 2, 80% \t train_loss: 0.0604 train_accuracy:90% took: 0.55s\n",
      "Epoch 2, 90% \t train_loss: 0.0587 train_accuracy:90% took: 0.67s\n",
      "Epoch 2, 100% \t train_loss: 0.0571 train_accuracy:90% took: 0.52s\n",
      "Test loss = 0.2638 Test Accuracy = 91%\n",
      "Epoch 3, 10% \t train_loss: 0.0566 train_accuracy:91% took: 0.82s\n",
      "Epoch 3, 20% \t train_loss: 0.0553 train_accuracy:91% took: 0.53s\n",
      "Epoch 3, 30% \t train_loss: 0.0518 train_accuracy:92% took: 0.74s\n",
      "Epoch 3, 40% \t train_loss: 0.0536 train_accuracy:92% took: 0.53s\n",
      "Epoch 3, 50% \t train_loss: 0.0517 train_accuracy:92% took: 0.52s\n",
      "Epoch 3, 60% \t train_loss: 0.0507 train_accuracy:92% took: 0.54s\n",
      "Epoch 3, 70% \t train_loss: 0.0490 train_accuracy:93% took: 0.87s\n",
      "Epoch 3, 80% \t train_loss: 0.0480 train_accuracy:92% took: 0.54s\n",
      "Epoch 3, 90% \t train_loss: 0.0517 train_accuracy:92% took: 0.59s\n",
      "Epoch 3, 100% \t train_loss: 0.0523 train_accuracy:91% took: 0.79s\n",
      "Test loss = 0.2137 Test Accuracy = 93%\n",
      "Epoch 4, 10% \t train_loss: 0.0434 train_accuracy:94% took: 0.77s\n",
      "Epoch 4, 20% \t train_loss: 0.0478 train_accuracy:91% took: 0.52s\n",
      "Epoch 4, 30% \t train_loss: 0.0501 train_accuracy:92% took: 0.72s\n",
      "Epoch 4, 40% \t train_loss: 0.0420 train_accuracy:94% took: 0.65s\n",
      "Epoch 4, 50% \t train_loss: 0.0452 train_accuracy:94% took: 0.53s\n",
      "Epoch 4, 60% \t train_loss: 0.0471 train_accuracy:93% took: 0.52s\n",
      "Epoch 4, 70% \t train_loss: 0.0444 train_accuracy:93% took: 0.52s\n",
      "Epoch 4, 80% \t train_loss: 0.0452 train_accuracy:94% took: 0.53s\n",
      "Epoch 4, 90% \t train_loss: 0.0471 train_accuracy:93% took: 0.71s\n",
      "Epoch 4, 100% \t train_loss: 0.0434 train_accuracy:94% took: 0.58s\n",
      "Test loss = 0.1848 Test Accuracy = 95%\n",
      "Epoch 5, 10% \t train_loss: 0.0378 train_accuracy:96% took: 0.78s\n",
      "Epoch 5, 20% \t train_loss: 0.0388 train_accuracy:95% took: 0.52s\n",
      "Epoch 5, 30% \t train_loss: 0.0436 train_accuracy:94% took: 0.53s\n",
      "Epoch 5, 40% \t train_loss: 0.0413 train_accuracy:93% took: 0.54s\n",
      "Epoch 5, 50% \t train_loss: 0.0438 train_accuracy:94% took: 0.51s\n",
      "Epoch 5, 60% \t train_loss: 0.0405 train_accuracy:94% took: 0.51s\n",
      "Epoch 5, 70% \t train_loss: 0.0427 train_accuracy:93% took: 0.68s\n",
      "Epoch 5, 80% \t train_loss: 0.0434 train_accuracy:93% took: 0.52s\n",
      "Epoch 5, 90% \t train_loss: 0.0401 train_accuracy:94% took: 0.53s\n",
      "Epoch 5, 100% \t train_loss: 0.0400 train_accuracy:94% took: 0.52s\n",
      "Test loss = 0.1827 Test Accuracy = 94%\n",
      "Epoch 6, 10% \t train_loss: 0.0348 train_accuracy:95% took: 0.75s\n",
      "Epoch 6, 20% \t train_loss: 0.0393 train_accuracy:94% took: 0.53s\n",
      "Epoch 6, 30% \t train_loss: 0.0387 train_accuracy:93% took: 0.52s\n",
      "Epoch 6, 40% \t train_loss: 0.0369 train_accuracy:95% took: 0.71s\n",
      "Epoch 6, 50% \t train_loss: 0.0395 train_accuracy:93% took: 0.52s\n",
      "Epoch 6, 60% \t train_loss: 0.0434 train_accuracy:93% took: 0.52s\n",
      "Epoch 6, 70% \t train_loss: 0.0360 train_accuracy:95% took: 0.52s\n",
      "Epoch 6, 80% \t train_loss: 0.0357 train_accuracy:96% took: 0.53s\n",
      "Epoch 6, 90% \t train_loss: 0.0348 train_accuracy:94% took: 0.52s\n",
      "Epoch 6, 100% \t train_loss: 0.0404 train_accuracy:93% took: 0.70s\n",
      "Test loss = 0.1617 Test Accuracy = 95%\n",
      "Epoch 7, 10% \t train_loss: 0.0338 train_accuracy:94% took: 0.82s\n",
      "Epoch 7, 20% \t train_loss: 0.0384 train_accuracy:94% took: 0.63s\n",
      "Epoch 7, 30% \t train_loss: 0.0413 train_accuracy:93% took: 0.52s\n",
      "Epoch 7, 40% \t train_loss: 0.0340 train_accuracy:94% took: 0.58s\n",
      "Epoch 7, 50% \t train_loss: 0.0324 train_accuracy:95% took: 0.53s\n",
      "Epoch 7, 60% \t train_loss: 0.0343 train_accuracy:95% took: 0.52s\n",
      "Epoch 7, 70% \t train_loss: 0.0349 train_accuracy:95% took: 0.54s\n",
      "Epoch 7, 80% \t train_loss: 0.0372 train_accuracy:93% took: 0.54s\n",
      "Epoch 7, 90% \t train_loss: 0.0341 train_accuracy:95% took: 0.65s\n",
      "Epoch 7, 100% \t train_loss: 0.0366 train_accuracy:93% took: 1.04s\n",
      "Test loss = 0.1497 Test Accuracy = 95%\n",
      "Epoch 8, 10% \t train_loss: 0.0365 train_accuracy:94% took: 0.75s\n",
      "Epoch 8, 20% \t train_loss: 0.0354 train_accuracy:94% took: 0.71s\n",
      "Epoch 8, 30% \t train_loss: 0.0341 train_accuracy:94% took: 0.64s\n",
      "Epoch 8, 40% \t train_loss: 0.0339 train_accuracy:94% took: 0.80s\n",
      "Epoch 8, 50% \t train_loss: 0.0331 train_accuracy:95% took: 0.59s\n",
      "Epoch 8, 60% \t train_loss: 0.0319 train_accuracy:95% took: 0.89s\n",
      "Epoch 8, 70% \t train_loss: 0.0331 train_accuracy:95% took: 0.61s\n",
      "Epoch 8, 80% \t train_loss: 0.0317 train_accuracy:95% took: 0.90s\n",
      "Epoch 8, 90% \t train_loss: 0.0336 train_accuracy:94% took: 0.76s\n",
      "Epoch 8, 100% \t train_loss: 0.0346 train_accuracy:94% took: 0.63s\n",
      "Test loss = 0.1478 Test Accuracy = 94%\n",
      "Epoch 9, 10% \t train_loss: 0.0328 train_accuracy:94% took: 1.06s\n",
      "Epoch 9, 20% \t train_loss: 0.0346 train_accuracy:94% took: 0.60s\n",
      "Epoch 9, 30% \t train_loss: 0.0307 train_accuracy:95% took: 0.61s\n",
      "Epoch 9, 40% \t train_loss: 0.0311 train_accuracy:96% took: 0.52s\n",
      "Epoch 9, 50% \t train_loss: 0.0384 train_accuracy:94% took: 0.53s\n",
      "Epoch 9, 60% \t train_loss: 0.0336 train_accuracy:94% took: 0.52s\n",
      "Epoch 9, 70% \t train_loss: 0.0314 train_accuracy:94% took: 0.52s\n",
      "Epoch 9, 80% \t train_loss: 0.0317 train_accuracy:94% took: 0.56s\n",
      "Epoch 9, 90% \t train_loss: 0.0281 train_accuracy:96% took: 0.52s\n",
      "Epoch 9, 100% \t train_loss: 0.0331 train_accuracy:95% took: 0.58s\n",
      "Test loss = 0.1504 Test Accuracy = 95%\n",
      "Epoch 10, 10% \t train_loss: 0.0321 train_accuracy:95% took: 0.76s\n",
      "Epoch 10, 20% \t train_loss: 0.0314 train_accuracy:95% took: 0.52s\n",
      "Epoch 10, 30% \t train_loss: 0.0333 train_accuracy:95% took: 0.54s\n",
      "Epoch 10, 40% \t train_loss: 0.0329 train_accuracy:94% took: 0.51s\n",
      "Epoch 10, 50% \t train_loss: 0.0309 train_accuracy:95% took: 0.53s\n",
      "Epoch 10, 60% \t train_loss: 0.0312 train_accuracy:94% took: 0.52s\n",
      "Epoch 10, 70% \t train_loss: 0.0288 train_accuracy:95% took: 0.51s\n",
      "Epoch 10, 80% \t train_loss: 0.0262 train_accuracy:96% took: 0.55s\n",
      "Epoch 10, 90% \t train_loss: 0.0304 train_accuracy:94% took: 0.59s\n",
      "Epoch 10, 100% \t train_loss: 0.0360 train_accuracy:94% took: 0.52s\n",
      "Test loss = 0.1536 Test Accuracy = 95%\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.00052046; weight_decay =0.00830640\n",
    "NN = Two_Layers_NN()\n",
    "train_net(NN, which_model = \"NN\", whether_norm = True, batch_size=5, n_epochs=10, learning_rate = learning_rate, \n",
    "              weight_decay = weight_decay, print_train_process = True, print_test_process = True, validation = True,loss_method = \"SGD\")\n",
    "del NN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight decay:0.0000 learning rate:0.00100000\n",
      "Epoch 1, 10% \t train_loss: 0.1384 train_accuracy:48% took: 0.84s\n",
      "Epoch 1, 20% \t train_loss: 0.1356 train_accuracy:54% took: 0.82s\n",
      "Epoch 1, 30% \t train_loss: 0.1336 train_accuracy:65% took: 0.69s\n",
      "Epoch 1, 40% \t train_loss: 0.1305 train_accuracy:70% took: 1.03s\n",
      "Epoch 1, 50% \t train_loss: 0.1269 train_accuracy:71% took: 0.74s\n",
      "Epoch 1, 60% \t train_loss: 0.1217 train_accuracy:73% took: 0.67s\n",
      "Epoch 1, 70% \t train_loss: 0.1100 train_accuracy:77% took: 0.69s\n",
      "Epoch 1, 80% \t train_loss: 0.1001 train_accuracy:80% took: 0.74s\n",
      "Epoch 1, 90% \t train_loss: 0.0911 train_accuracy:84% took: 0.70s\n",
      "Epoch 1, 100% \t train_loss: 0.0859 train_accuracy:83% took: 0.82s\n",
      "weight decay:0.0010 learning rate:0.00100000\n",
      "Epoch 1, 10% \t train_loss: 0.1368 train_accuracy:55% took: 0.76s\n",
      "Epoch 1, 20% \t train_loss: 0.1345 train_accuracy:67% took: 0.70s\n",
      "Epoch 1, 30% \t train_loss: 0.1292 train_accuracy:70% took: 0.85s\n",
      "Epoch 1, 40% \t train_loss: 0.1275 train_accuracy:69% took: 0.83s\n",
      "Epoch 1, 50% \t train_loss: 0.1219 train_accuracy:72% took: 0.79s\n",
      "Epoch 1, 60% \t train_loss: 0.1154 train_accuracy:73% took: 0.87s\n",
      "Epoch 1, 70% \t train_loss: 0.1102 train_accuracy:73% took: 0.73s\n",
      "Epoch 1, 80% \t train_loss: 0.0984 train_accuracy:82% took: 0.68s\n",
      "Epoch 1, 90% \t train_loss: 0.0884 train_accuracy:84% took: 0.75s\n",
      "Epoch 1, 100% \t train_loss: 0.0760 train_accuracy:86% took: 0.72s\n",
      "weight decay:10000.0000 learning rate:0.00100000\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "loss explode due to large regularization or learning rate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-c846da48a923>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mLN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLeNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m train_net(LN, which_model = \"LeNet\", whether_norm = True, batch_size=5, n_epochs=1, learning_rate = learning_rate, \n\u001b[0;32m---> 22\u001b[0;31m               weight_decay = weight_decay, print_train_process = True, print_test_process = False, validation = False,loss_method = \"SGD\")\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mLN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-373f73c14a16>\u001b[0m in \u001b[0;36mtrain_net\u001b[0;34m(net, loss_method, which_model, whether_norm, batch_size, n_epochs, learning_rate, weight_decay, print_train_process, print_test_process, validation)\u001b[0m\n\u001b[1;32m    207\u001b[0m             \u001b[0mloss_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_size\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loss explode due to large regularization or learning rate\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m             \u001b[0mloss_size\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: loss explode due to large regularization or learning rate"
     ]
    }
   ],
   "source": [
    "#sanity check\n",
    "weight_decay = 0; learning_rate = 0.001\n",
    "print(\"weight decay:{:.4f} learning rate:{:.8f}\".format(weight_decay, learning_rate))\n",
    "LN = LeNet()\n",
    "train_net(LN, which_model = \"LeNet\", whether_norm = True, batch_size=5, n_epochs=1, learning_rate = learning_rate, \n",
    "              weight_decay = weight_decay, print_train_process = True, print_test_process = False, validation = False,loss_method = \"SGD\")\n",
    "    \n",
    "del LN\n",
    "\n",
    "weight_decay = 0.001; learning_rate = 0.001\n",
    "print(\"weight decay:{:.4f} learning rate:{:.8f}\".format(weight_decay, learning_rate))\n",
    "LN = LeNet()\n",
    "train_net(LN, which_model = \"LeNet\", whether_norm = True, batch_size=5, n_epochs=1, learning_rate = learning_rate, \n",
    "              weight_decay = weight_decay, print_train_process = True, print_test_process = False, validation = False,loss_method = \"SGD\")\n",
    "    \n",
    "del LN\n",
    "\n",
    "weight_decay = 10000; learning_rate = 0.001\n",
    "print(\"weight decay:{:.4f} learning rate:{:.8f}\".format(weight_decay, learning_rate))\n",
    "LN = LeNet()\n",
    "train_net(LN, which_model = \"LeNet\", whether_norm = True, batch_size=5, n_epochs=1, learning_rate = learning_rate, \n",
    "              weight_decay = weight_decay, print_train_process = True, print_test_process = False, validation = False,loss_method = \"SGD\")\n",
    "    \n",
    "del LN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As regularization increases, loss also increases. When regularization is too large, loss explodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight decay:0.0010 learning rate:0.00001000\n",
      "Epoch 1, 10% \t train_loss: 0.1412 train_accuracy:31% took: 0.81s\n",
      "Epoch 1, 20% \t train_loss: 0.1408 train_accuracy:33% took: 0.69s\n",
      "Epoch 1, 30% \t train_loss: 0.1411 train_accuracy:32% took: 0.69s\n",
      "Epoch 1, 40% \t train_loss: 0.1409 train_accuracy:33% took: 0.70s\n",
      "Epoch 1, 50% \t train_loss: 0.1410 train_accuracy:33% took: 0.70s\n",
      "Epoch 1, 60% \t train_loss: 0.1405 train_accuracy:36% took: 0.70s\n",
      "Epoch 1, 70% \t train_loss: 0.1408 train_accuracy:34% took: 0.68s\n",
      "Epoch 1, 80% \t train_loss: 0.1406 train_accuracy:35% took: 0.67s\n",
      "Epoch 1, 90% \t train_loss: 0.1406 train_accuracy:36% took: 0.68s\n",
      "Epoch 1, 100% \t train_loss: 0.1406 train_accuracy:34% took: 0.81s\n",
      "weight decay:0.0010 learning rate:0.00100000\n",
      "Epoch 1, 10% \t train_loss: 0.1383 train_accuracy:46% took: 0.74s\n",
      "Epoch 1, 20% \t train_loss: 0.1358 train_accuracy:60% took: 0.69s\n",
      "Epoch 1, 30% \t train_loss: 0.1333 train_accuracy:71% took: 0.68s\n",
      "Epoch 1, 40% \t train_loss: 0.1297 train_accuracy:72% took: 0.69s\n",
      "Epoch 1, 50% \t train_loss: 0.1251 train_accuracy:72% took: 0.68s\n",
      "Epoch 1, 60% \t train_loss: 0.1178 train_accuracy:75% took: 0.67s\n",
      "Epoch 1, 70% \t train_loss: 0.1138 train_accuracy:71% took: 0.72s\n",
      "Epoch 1, 80% \t train_loss: 0.1044 train_accuracy:75% took: 0.89s\n",
      "Epoch 1, 90% \t train_loss: 0.0950 train_accuracy:80% took: 1.00s\n",
      "Epoch 1, 100% \t train_loss: 0.0896 train_accuracy:82% took: 0.75s\n",
      "weight decay:0.0010 learning rate:0.10000000\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "loss explode due to large regularization or learning rate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-e4af65fb3672>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mLN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLeNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m train_net(LN, which_model = \"LeNet\", whether_norm = True, batch_size=5, n_epochs=1, learning_rate = learning_rate, \n\u001b[0;32m---> 22\u001b[0;31m               weight_decay = weight_decay, print_train_process = True, print_test_process = False, validation = False,loss_method = \"SGD\")\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mLN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-373f73c14a16>\u001b[0m in \u001b[0;36mtrain_net\u001b[0;34m(net, loss_method, which_model, whether_norm, batch_size, n_epochs, learning_rate, weight_decay, print_train_process, print_test_process, validation)\u001b[0m\n\u001b[1;32m    207\u001b[0m             \u001b[0mloss_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_size\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loss explode due to large regularization or learning rate\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m             \u001b[0mloss_size\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: loss explode due to large regularization or learning rate"
     ]
    }
   ],
   "source": [
    "\n",
    "weight_decay = 0.001; learning_rate = 0.00001\n",
    "print(\"weight decay:{:.4f} learning rate:{:.8f}\".format(weight_decay, learning_rate))\n",
    "LN = LeNet()\n",
    "train_net(LN, which_model = \"LeNet\", whether_norm = True, batch_size=5, n_epochs=1, learning_rate = learning_rate, \n",
    "              weight_decay = weight_decay, print_train_process = True, print_test_process = False, validation = False,loss_method = \"SGD\")\n",
    "    \n",
    "del LN\n",
    "\n",
    "weight_decay = 0.001; learning_rate = 0.001\n",
    "print(\"weight decay:{:.4f} learning rate:{:.8f}\".format(weight_decay, learning_rate))\n",
    "LN = LeNet()\n",
    "train_net(LN, which_model = \"LeNet\", whether_norm = True, batch_size=5, n_epochs=1, learning_rate = learning_rate, \n",
    "              weight_decay = weight_decay, print_train_process = True, print_test_process = False, validation = False,loss_method = \"SGD\")\n",
    "    \n",
    "del LN\n",
    "\n",
    "weight_decay = 0.001; learning_rate = 0.1\n",
    "print(\"weight decay:{:.4f} learning rate:{:.8f}\".format(weight_decay, learning_rate))\n",
    "LN = LeNet()\n",
    "train_net(LN, which_model = \"LeNet\", whether_norm = True, batch_size=5, n_epochs=1, learning_rate = learning_rate, \n",
    "              weight_decay = weight_decay, print_train_process = True, print_test_process = False, validation = False,loss_method = \"SGD\")\n",
    "    \n",
    "del LN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When learning rate is too small, loss barely changes. When learning rate is too large, loss explodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter Optimization(random search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss = 0.0937 Test Accuracy = 97%\n",
      "train_loss: 0.02109754 train_accuracy:96% learning rate:0.00582210 regularization:0.00007245 running time:43.6651\n",
      "Test loss = 0.2049 Test Accuracy = 92%\n",
      "train_loss: 0.04114626 train_accuracy:92% learning rate:0.00043608 regularization:0.00243426 running time:39.1342\n",
      "Test loss = 0.5368 Test Accuracy = 75%\n",
      "train_loss: 0.10642646 train_accuracy:75% learning rate:0.00012184 regularization:0.00000132 running time:41.3061\n",
      "Test loss = 0.1220 Test Accuracy = 95%\n",
      "train_loss: 0.02056650 train_accuracy:96% learning rate:0.00531958 regularization:0.00012685 running time:43.9437\n",
      "Test loss = 0.4507 Test Accuracy = 83%\n",
      "train_loss: 0.09391274 train_accuracy:81% learning rate:0.00019317 regularization:0.00000188 running time:44.8019\n",
      "Test loss = 0.1494 Test Accuracy = 95%\n",
      "train_loss: 0.02318160 train_accuracy:95% learning rate:0.00367561 regularization:0.00048379 running time:44.1863\n",
      "Test loss = 0.6241 Test Accuracy = 71%\n",
      "train_loss: 0.12537603 train_accuracy:73% learning rate:0.00006051 regularization:0.00003437 running time:41.8498\n",
      "Test loss = 0.6833 Test Accuracy = 54%\n",
      "train_loss: 0.13626477 train_accuracy:54% learning rate:0.00001902 regularization:0.00005666 running time:45.4452\n",
      "Test loss = 0.1799 Test Accuracy = 94%\n",
      "train_loss: 0.03628243 train_accuracy:92% learning rate:0.00064598 regularization:0.00000743 running time:43.1933\n",
      "Test loss = 0.1173 Test Accuracy = 95%\n",
      "train_loss: 0.02536073 train_accuracy:95% learning rate:0.00314349 regularization:0.00357941 running time:41.4396\n"
     ]
    }
   ],
   "source": [
    "for count in range(10):\n",
    "    learning_rate = 10 ** random.uniform(-5,-2)\n",
    "    weight_decay = 10 ** random.uniform(-6,-1)\n",
    "    \n",
    "    LN = LeNet()\n",
    "    \n",
    "    try:\n",
    "        train_net(LN, which_model = \"LeNet\", whether_norm = True, batch_size=5, n_epochs=5, learning_rate = learning_rate, \n",
    "              weight_decay = weight_decay, print_train_process = False, print_test_process = False, validation = True,loss_method = \"SGD\")\n",
    "    except:\n",
    "        print(\"loss explodes. learning rate:{:.8f} regularization:{:.8f}\".format(learning_rate, weight_decay))\n",
    "        \n",
    "    del LN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose the pair with highest accuracy rate and smallest loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, 10% \t train_loss: 0.1324 train_accuracy:60% took: 0.79s\n",
      "Epoch 1, 20% \t train_loss: 0.0987 train_accuracy:78% took: 0.73s\n",
      "Epoch 1, 30% \t train_loss: 0.0554 train_accuracy:89% took: 0.69s\n",
      "Epoch 1, 40% \t train_loss: 0.0419 train_accuracy:92% took: 0.72s\n",
      "Epoch 1, 50% \t train_loss: 0.0417 train_accuracy:92% took: 0.71s\n",
      "Epoch 1, 60% \t train_loss: 0.0389 train_accuracy:91% took: 0.69s\n",
      "Epoch 1, 70% \t train_loss: 0.0342 train_accuracy:92% took: 0.70s\n",
      "Epoch 1, 80% \t train_loss: 0.0334 train_accuracy:93% took: 0.70s\n",
      "Epoch 1, 90% \t train_loss: 0.0357 train_accuracy:93% took: 0.71s\n",
      "Epoch 1, 100% \t train_loss: 0.0323 train_accuracy:93% took: 0.71s\n",
      "Test loss = 0.1423 Test Accuracy = 94%\n",
      "Epoch 2, 10% \t train_loss: 0.0256 train_accuracy:94% took: 1.04s\n",
      "Epoch 2, 20% \t train_loss: 0.0308 train_accuracy:94% took: 0.84s\n",
      "Epoch 2, 30% \t train_loss: 0.0320 train_accuracy:93% took: 0.81s\n",
      "Epoch 2, 40% \t train_loss: 0.0309 train_accuracy:94% took: 0.71s\n",
      "Epoch 2, 50% \t train_loss: 0.0291 train_accuracy:93% took: 0.72s\n",
      "Epoch 2, 60% \t train_loss: 0.0248 train_accuracy:94% took: 0.69s\n",
      "Epoch 2, 70% \t train_loss: 0.0253 train_accuracy:95% took: 0.71s\n",
      "Epoch 2, 80% \t train_loss: 0.0272 train_accuracy:94% took: 0.74s\n",
      "Epoch 2, 90% \t train_loss: 0.0293 train_accuracy:95% took: 0.87s\n",
      "Epoch 2, 100% \t train_loss: 0.0234 train_accuracy:96% took: 0.82s\n",
      "Test loss = 0.1142 Test Accuracy = 96%\n",
      "Epoch 3, 10% \t train_loss: 0.0270 train_accuracy:94% took: 1.11s\n",
      "Epoch 3, 20% \t train_loss: 0.0286 train_accuracy:94% took: 0.79s\n",
      "Epoch 3, 30% \t train_loss: 0.0217 train_accuracy:95% took: 0.74s\n",
      "Epoch 3, 40% \t train_loss: 0.0199 train_accuracy:96% took: 0.74s\n",
      "Epoch 3, 50% \t train_loss: 0.0248 train_accuracy:95% took: 0.74s\n",
      "Epoch 3, 60% \t train_loss: 0.0284 train_accuracy:95% took: 0.88s\n",
      "Epoch 3, 70% \t train_loss: 0.0196 train_accuracy:96% took: 0.80s\n",
      "Epoch 3, 80% \t train_loss: 0.0252 train_accuracy:94% took: 0.74s\n",
      "Epoch 3, 90% \t train_loss: 0.0226 train_accuracy:95% took: 0.71s\n",
      "Epoch 3, 100% \t train_loss: 0.0289 train_accuracy:94% took: 0.79s\n",
      "Test loss = 0.1144 Test Accuracy = 95%\n",
      "Epoch 4, 10% \t train_loss: 0.0258 train_accuracy:95% took: 1.09s\n",
      "Epoch 4, 20% \t train_loss: 0.0239 train_accuracy:95% took: 0.76s\n",
      "Epoch 4, 30% \t train_loss: 0.0202 train_accuracy:96% took: 0.74s\n",
      "Epoch 4, 40% \t train_loss: 0.0252 train_accuracy:94% took: 0.74s\n",
      "Epoch 4, 50% \t train_loss: 0.0231 train_accuracy:95% took: 0.76s\n",
      "Epoch 4, 60% \t train_loss: 0.0181 train_accuracy:96% took: 0.86s\n",
      "Epoch 4, 70% \t train_loss: 0.0235 train_accuracy:96% took: 0.86s\n",
      "Epoch 4, 80% \t train_loss: 0.0240 train_accuracy:96% took: 0.77s\n",
      "Epoch 4, 90% \t train_loss: 0.0187 train_accuracy:96% took: 0.75s\n",
      "Epoch 4, 100% \t train_loss: 0.0205 train_accuracy:96% took: 0.73s\n",
      "Test loss = 0.1025 Test Accuracy = 96%\n",
      "Epoch 5, 10% \t train_loss: 0.0230 train_accuracy:95% took: 1.01s\n",
      "Epoch 5, 20% \t train_loss: 0.0230 train_accuracy:95% took: 0.87s\n",
      "Epoch 5, 30% \t train_loss: 0.0176 train_accuracy:96% took: 0.81s\n",
      "Epoch 5, 40% \t train_loss: 0.0242 train_accuracy:95% took: 0.73s\n",
      "Epoch 5, 50% \t train_loss: 0.0222 train_accuracy:95% took: 0.75s\n",
      "Epoch 5, 60% \t train_loss: 0.0156 train_accuracy:96% took: 0.74s\n",
      "Epoch 5, 70% \t train_loss: 0.0182 train_accuracy:96% took: 0.76s\n",
      "Epoch 5, 80% \t train_loss: 0.0265 train_accuracy:96% took: 0.74s\n",
      "Epoch 5, 90% \t train_loss: 0.0227 train_accuracy:95% took: 0.76s\n",
      "Epoch 5, 100% \t train_loss: 0.0212 train_accuracy:96% took: 0.75s\n",
      "Test loss = 0.0909 Test Accuracy = 95%\n",
      "Epoch 6, 10% \t train_loss: 0.0170 train_accuracy:96% took: 1.02s\n",
      "Epoch 6, 20% \t train_loss: 0.0189 train_accuracy:96% took: 0.73s\n",
      "Epoch 6, 30% \t train_loss: 0.0176 train_accuracy:97% took: 0.89s\n",
      "Epoch 6, 40% \t train_loss: 0.0205 train_accuracy:96% took: 0.80s\n",
      "Epoch 6, 50% \t train_loss: 0.0182 train_accuracy:96% took: 0.74s\n",
      "Epoch 6, 60% \t train_loss: 0.0203 train_accuracy:96% took: 0.76s\n",
      "Epoch 6, 70% \t train_loss: 0.0272 train_accuracy:94% took: 0.76s\n",
      "Epoch 6, 80% \t train_loss: 0.0207 train_accuracy:96% took: 0.75s\n",
      "Epoch 6, 90% \t train_loss: 0.0175 train_accuracy:96% took: 0.78s\n",
      "Epoch 6, 100% \t train_loss: 0.0271 train_accuracy:94% took: 0.76s\n",
      "Test loss = 0.0812 Test Accuracy = 97%\n",
      "Epoch 7, 10% \t train_loss: 0.0163 train_accuracy:96% took: 0.99s\n",
      "Epoch 7, 20% \t train_loss: 0.0188 train_accuracy:96% took: 0.76s\n",
      "Epoch 7, 30% \t train_loss: 0.0204 train_accuracy:96% took: 0.87s\n",
      "Epoch 7, 40% \t train_loss: 0.0140 train_accuracy:97% took: 0.74s\n",
      "Epoch 7, 50% \t train_loss: 0.0136 train_accuracy:97% took: 0.81s\n",
      "Epoch 7, 60% \t train_loss: 0.0240 train_accuracy:95% took: 0.89s\n",
      "Epoch 7, 70% \t train_loss: 0.0213 train_accuracy:96% took: 0.97s\n",
      "Epoch 7, 80% \t train_loss: 0.0187 train_accuracy:96% took: 0.74s\n",
      "Epoch 7, 90% \t train_loss: 0.0224 train_accuracy:95% took: 0.74s\n",
      "Epoch 7, 100% \t train_loss: 0.0243 train_accuracy:94% took: 0.72s\n",
      "Test loss = 0.0873 Test Accuracy = 95%\n",
      "Epoch 8, 10% \t train_loss: 0.0201 train_accuracy:96% took: 1.03s\n",
      "Epoch 8, 20% \t train_loss: 0.0181 train_accuracy:97% took: 0.84s\n",
      "Epoch 8, 30% \t train_loss: 0.0181 train_accuracy:96% took: 0.74s\n",
      "Epoch 8, 40% \t train_loss: 0.0215 train_accuracy:95% took: 0.75s\n",
      "Epoch 8, 50% \t train_loss: 0.0128 train_accuracy:97% took: 0.96s\n",
      "Epoch 8, 60% \t train_loss: 0.0259 train_accuracy:95% took: 0.76s\n",
      "Epoch 8, 70% \t train_loss: 0.0188 train_accuracy:96% took: 0.75s\n",
      "Epoch 8, 80% \t train_loss: 0.0168 train_accuracy:96% took: 0.77s\n",
      "Epoch 8, 90% \t train_loss: 0.0140 train_accuracy:97% took: 0.73s\n",
      "Epoch 8, 100% \t train_loss: 0.0199 train_accuracy:96% took: 0.71s\n",
      "Test loss = 0.0960 Test Accuracy = 95%\n",
      "Epoch 9, 10% \t train_loss: 0.0190 train_accuracy:96% took: 0.97s\n",
      "Epoch 9, 20% \t train_loss: 0.0164 train_accuracy:96% took: 0.74s\n",
      "Epoch 9, 30% \t train_loss: 0.0205 train_accuracy:96% took: 0.75s\n",
      "Epoch 9, 40% \t train_loss: 0.0193 train_accuracy:96% took: 1.01s\n",
      "Epoch 9, 50% \t train_loss: 0.0166 train_accuracy:96% took: 0.99s\n",
      "Epoch 9, 60% \t train_loss: 0.0183 train_accuracy:96% took: 0.92s\n",
      "Epoch 9, 70% \t train_loss: 0.0156 train_accuracy:96% took: 1.15s\n",
      "Epoch 9, 80% \t train_loss: 0.0221 train_accuracy:96% took: 0.78s\n",
      "Epoch 9, 90% \t train_loss: 0.0179 train_accuracy:96% took: 0.75s\n",
      "Epoch 9, 100% \t train_loss: 0.0137 train_accuracy:97% took: 0.72s\n",
      "Test loss = 0.0890 Test Accuracy = 97%\n",
      "Epoch 10, 10% \t train_loss: 0.0191 train_accuracy:96% took: 1.03s\n",
      "Epoch 10, 20% \t train_loss: 0.0117 train_accuracy:98% took: 0.73s\n",
      "Epoch 10, 30% \t train_loss: 0.0161 train_accuracy:96% took: 0.74s\n",
      "Epoch 10, 40% \t train_loss: 0.0175 train_accuracy:96% took: 0.74s\n",
      "Epoch 10, 50% \t train_loss: 0.0157 train_accuracy:97% took: 0.95s\n",
      "Epoch 10, 60% \t train_loss: 0.0171 train_accuracy:97% took: 0.88s\n",
      "Epoch 10, 70% \t train_loss: 0.0195 train_accuracy:96% took: 0.82s\n",
      "Epoch 10, 80% \t train_loss: 0.0159 train_accuracy:96% took: 0.92s\n",
      "Epoch 10, 90% \t train_loss: 0.0216 train_accuracy:96% took: 0.87s\n",
      "Epoch 10, 100% \t train_loss: 0.0143 train_accuracy:97% took: 0.87s\n",
      "Test loss = 0.0890 Test Accuracy = 96%\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.00582210; weight_decay = 0.00007245\n",
    "LN = LeNet()\n",
    "train_net(LN, which_model = \"LeNet\", whether_norm = True, batch_size=5, n_epochs=10, learning_rate = learning_rate, \n",
    "              weight_decay = weight_decay, print_train_process = True, print_test_process = True, validation = True,loss_method = \"SGD\")\n",
    "del LN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
